{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Restore spotted cornea image with inpainting\n\nOptical coherence tomography (OCT) is a non-invasive imaging technique used by\nophthalmologists to take pictures of the back of a patient's eye [1]_.\nWhen performing OCT,\ndust may stick to the reference mirror of the equipment, causing dark spots to\nappear on the images. The problem is that these dirt spots cover areas of\nin-vivo tissue, hence hiding data of interest. Our goal here is to restore\n(reconstruct) the hidden areas based on the pixels near their boundaries.\n\nThis tutorial is adapted from an application shared by Jules Scholler [2]_.\nThe images were acquired by Viacheslav Mazlin (see\n:func:`skimage.data.palisades_of_vogt`).\n\n.. [1] David Turbert, reviewed by Ninel Z Gregori, MD (2023)\n       [What Is Optical Coherence Tomography?](https://www.aao.org/eye-health/treatments/what-is-optical-coherence-tomography),\n       American Academy of Ophthalmology.\n.. [2] Jules Scholler (2019) \"Image denoising using inpainting\"\n       https://www.jscholler.com/2019-02-28-remove-dots/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.io\nimport plotly.express as px\n\nimport skimage as ski"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset we are using here is an image sequence (a movie!) of\nhuman in-vivo tissue. Specifically, it shows the *palisades of Vogt* of a\ngiven cornea sample.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load image data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_seq = ski.data.palisades_of_vogt()\n\nprint(f'number of dimensions: {image_seq.ndim}')\nprint(f'shape: {image_seq.shape}')\nprint(f'dtype: {image_seq.dtype}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is an image stack with 60 frames (time points) and 2 spatial\ndimensions. Let us visualize 10 frames by sampling every six time points:\nWe can see some changes in illumination.\nWe take advantage of the ``animation_frame`` parameter in\nPlotly's ``imshow`` function. As a side note, when the\n``binary_string`` parameter is set to ``True``, the image is\nrepresented as grayscale.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(\n    image_seq[::6, :, :],\n    animation_frame=0,\n    binary_string=True,\n    labels={'animation_frame': '6-step time point'},\n    title='Sample of in-vivo human cornea',\n)\nfig.update_layout(autosize=False, minreducedwidth=250, minreducedheight=250)\nplotly.io.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate over time\nFirst, we want to detect those dirt spots where the data are lost. In\ntechnical terms, we want to *segment* the dirt spots (for\nall frames in the sequence). Unlike the actual data (signal), the dirt spots\ndo not move from one frame to the next; they are still. Therefore, we begin\nby computing a time aggregate of the image sequence. We shall use the median\nimage to segment the dirt spots, the latter then standing out\nwith respect to the background (blurred signal).\nComplementarily, to get a feel for the (moving) data, let us compute the\nvariance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_med = np.median(image_seq, axis=0)\nimage_var = np.var(image_seq, axis=0)\n\nassert image_var.shape == image_med.shape\n\nprint(f'shape: {image_med.shape}')\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n\nax[0].imshow(image_med, cmap='gray')\nax[0].set_title('Image median over time')\nax[1].imshow(image_var, cmap='gray')\nax[1].set_title('Image variance over time')\n\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use local thresholding\nTo segment the dirt spots, we use thresholding. The images we are working\nwith are unevenly illuminated, which causes spatial variations in the\n(absolute) intensities of the foreground and the background. For example,\nthe average background intensity in one region may be different in another\n(distant) one. It is therefore more fitting to compute different threshold\nvalues across the image, one for each region. This is called adaptive (or\nlocal) thresholding, as opposed to the usual thresholding procedure which\nemploys a single (global) threshold for all pixels in the image.\n\nWhen calling the ``threshold_local`` function from the ``filters`` module,\nwe may change the default neighborhood size (``block_size``), i.e., the\ntypical size (number of pixels) over which illumination varies,\nas well as the ``offset`` (shifting the neighborhood's weighted mean).\nLet us try two different values for ``block_size``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thresh_1 = ski.filters.threshold_local(image_med, block_size=21, offset=15)\nthresh_2 = ski.filters.threshold_local(image_med, block_size=43, offset=15)\n\nmask_1 = image_med < thresh_1\nmask_2 = image_med < thresh_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us define a convenience function to display two plots side by side, so\nit is easier for us to compare them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_comparison(plot1, plot2, title1, title2):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6), sharex=True, sharey=True)\n    ax1.imshow(plot1, cmap='gray')\n    ax1.set_title(title1)\n    ax2.imshow(plot2, cmap='gray')\n    ax2.set_title(title2)\n\n\nplot_comparison(mask_1, mask_2, \"block_size = 21\", \"block_size = 43\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The \"dirt spots\" appear to be more distinct in the second mask, i.e., the\none resulting from using the larger ``block_size`` value.\nWe noticed that increasing the value of the offset parameter from\nits default zero value would yield a more uniform background,\nletting the objects of interest stand out more visibly. Note that\ntoggling parameter values can give us a deeper\nunderstanding of the method being used, which can typically move us\ncloser to the desired results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thresh_0 = ski.filters.threshold_local(image_med, block_size=43)\n\nmask_0 = image_med < thresh_0\n\nplot_comparison(mask_0, mask_2, \"No offset\", \"Offset = 15\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove fine-grained features\nWe use morphological filters to sharpen the mask and focus on the dirt\nspots. The two fundamental morphological operators are *dilation* and\n*erosion*, where dilation (resp. erosion) sets the pixel to the brightest\n(resp. darkest) value of the neighborhood defined by a structuring element\n(footprint).\n\nHere, we use the ``diamond`` function from the ``morphology`` module to\ncreate a diamond-shaped footprint.\nAn erosion followed by a dilation is called an *opening*.\nFirst, we apply an opening filter, in order to remove small objects and thin\nlines, while preserving the shape and size of larger objects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "footprint = ski.morphology.diamond(3)\nmask_open = ski.morphology.opening(mask_2, footprint)\nplot_comparison(mask_2, mask_open, \"mask before\", \"after opening\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since \"opening\" an image starts with an erosion operation, bright regions\nwhich are smaller than the structuring element have been removed.\nWhen applying an opening filter, tweaking the footprint parameter lets us\ncontrol how fine-grained the removed features are. For example, if we used\n``footprint = ski.morphology.diamond(1)`` in the above, we could see that\nonly smaller features would be filtered out, hence retaining more spots in\nthe mask. Conversely, if we used a disk-shaped footprint of same radius,\ni.e., ``footprint = ski.morphology.disk(3)``, more of the fine-grained\nfeatures would be filtered out, since the disk's area is larger than the\ndiamond's.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can make the detected areas wider by applying a dilation filter:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask_dilate = ski.morphology.dilation(mask_open, footprint)\nplot_comparison(mask_open, mask_dilate, \"Before\", \"After dilation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dilation enlarges bright regions and shrinks dark regions.\nNotice how, indeed, the white spots have thickened.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inpaint each frame separately\nWe are now ready to apply inpainting to each frame. For this we use function\n``inpaint_biharmonic`` from the ``restoration`` module. It implements an\nalgorithm based on biharmonic equations.\nThis function takes two arrays as inputs:\nThe image to restore and a mask (with same shape) corresponding to the\nregions we want to inpaint.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_seq_inpainted = np.zeros(image_seq.shape)\n\nfor i in range(image_seq.shape[0]):\n    image_seq_inpainted[i] = ski.restoration.inpaint_biharmonic(\n        image_seq[i], mask_dilate\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us visualize one restored image, where the dirt spots have been\ninpainted. First, we find the contours of the dirt spots (well, of the mask)\nso we can draw them on top of the restored image:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contours = ski.measure.find_contours(mask_dilate)\n\n# Gather all (row, column) coordinates of the contours\nx = []\ny = []\nfor contour in contours:\n    x.append(contour[:, 0])\n    y.append(contour[:, 1])\n# Note that the following one-liner is equivalent to the above:\n# x, y = zip(*((contour[:, 0], contour[:, 1]) for contour in contours))\n\n# Flatten the coordinates\nx_flat = np.concatenate(x).ravel().round().astype(int)\ny_flat = np.concatenate(y).ravel().round().astype(int)\n# Create mask of these contours\ncontour_mask = np.zeros(mask_dilate.shape, dtype=bool)\ncontour_mask[x_flat, y_flat] = 1\n# Pick one frame\nsample_result = image_seq_inpainted[12]\n# Normalize it (so intensity values range [0, 1])\nsample_result /= sample_result.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use function ``label2rgb`` from the ``color`` module to overlay the\nrestored image with the segmented spots, using transparency (alpha\nparameter).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "color_contours = ski.color.label2rgb(\n    contour_mask, image=sample_result, alpha=0.4, bg_color=(1, 1, 1)\n)\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nax.imshow(color_contours)\nax.set_title('Segmented spots over restored image')\n\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the dirt spot located at (x, y) ~ (719, 1237) stands out; ideally,\nit should have been segmented and inpainted. We can see that we 'lost' it to\nthe opening processing step, when removing fine-grained features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}