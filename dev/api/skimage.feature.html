
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>skimage.feature &#8212; skimage 0.25.0rc1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/theme_overrides.css?v=4340df76" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=ce5c87ce"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script data-domain="scikit-image.org" defer="defer" src="https://views.scientific-python.org/js/script.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/skimage.feature';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://scikit-image.org/docs/dev/_static/version_switcher.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '0.25.0rc1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="skimage.filters" href="skimage.filters.html" />
    <link rel="prev" title="skimage.exposure" href="skimage.exposure.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
     
  

<a class="navbar-brand logo" href="https://scikit-image.org">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="scikit-image's logo, showing a snake's head overlayed with green and orange"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="scikit-image's logo, showing a snake's head overlayed with green and orange"/>`);</script>
  
  
    <p class="title logo__title">scikit-image</p>
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../release_notes/index.html">
    Release notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/index.html">
    About
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-image/scikit-image" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/scikit-image/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-box fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide/index.html">
    User guide
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="api.html">
    API reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../release_notes/index.html">
    Release notes
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about/index.html">
    About
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-image/scikit-image" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/scikit-image/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-box fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="skimage.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.color.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.color</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.data.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.draw.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.draw</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.exposure.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.exposure</span></code></a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.feature</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.filters.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.filters</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.filters.rank.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.filters.rank</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.future.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.future</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.graph.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.graph</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.io.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.io</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.measure.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.measure</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.metrics.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.metrics</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.morphology.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.morphology</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.registration.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.registration</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.restoration.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.restoration</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.segmentation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.segmentation</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.transform.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.transform</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="skimage.util.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.util</span></code></a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="api.html" class="nav-link">API reference</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><code...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="module-skimage.feature">
<span id="skimage-feature"></span><h1><a class="reference internal" href="#module-skimage.feature" title="skimage.feature"><code class="xref py py-mod docutils literal notranslate"><span class="pre">skimage.feature</span></code></a><a class="headerlink" href="#module-skimage.feature" title="Link to this heading">#</a></h1>
<div class="pst-scrollable-table-container"><table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.blob_dog" title="skimage.feature.blob_dog"><code class="xref py py-obj docutils literal notranslate"><span class="pre">blob_dog</span></code></a></p></td>
<td><p>Finds blobs in the given grayscale image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.blob_doh" title="skimage.feature.blob_doh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">blob_doh</span></code></a></p></td>
<td><p>Finds blobs in the given grayscale image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.blob_log" title="skimage.feature.blob_log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">blob_log</span></code></a></p></td>
<td><p>Finds blobs in the given grayscale image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.canny" title="skimage.feature.canny"><code class="xref py py-obj docutils literal notranslate"><span class="pre">canny</span></code></a></p></td>
<td><p>Edge filter an image using the Canny algorithm.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.corner_fast" title="skimage.feature.corner_fast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_fast</span></code></a></p></td>
<td><p>Extract FAST corners for a given image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.corner_foerstner" title="skimage.feature.corner_foerstner"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_foerstner</span></code></a></p></td>
<td><p>Compute Foerstner corner measure response image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.corner_harris" title="skimage.feature.corner_harris"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_harris</span></code></a></p></td>
<td><p>Compute Harris corner measure response image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.corner_kitchen_rosenfeld" title="skimage.feature.corner_kitchen_rosenfeld"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_kitchen_rosenfeld</span></code></a></p></td>
<td><p>Compute Kitchen and Rosenfeld corner measure response image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.corner_moravec" title="skimage.feature.corner_moravec"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_moravec</span></code></a></p></td>
<td><p>Compute Moravec corner measure response image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.corner_orientations" title="skimage.feature.corner_orientations"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_orientations</span></code></a></p></td>
<td><p>Compute the orientation of corners.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.corner_peaks" title="skimage.feature.corner_peaks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_peaks</span></code></a></p></td>
<td><p>Find peaks in corner measure response image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.corner_shi_tomasi" title="skimage.feature.corner_shi_tomasi"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_shi_tomasi</span></code></a></p></td>
<td><p>Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.corner_subpix" title="skimage.feature.corner_subpix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">corner_subpix</span></code></a></p></td>
<td><p>Determine subpixel position of corners.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.daisy" title="skimage.feature.daisy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">daisy</span></code></a></p></td>
<td><p>Extract DAISY feature descriptors densely for the given image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.draw_haar_like_feature" title="skimage.feature.draw_haar_like_feature"><code class="xref py py-obj docutils literal notranslate"><span class="pre">draw_haar_like_feature</span></code></a></p></td>
<td><p>Visualization of Haar-like features.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.draw_multiblock_lbp" title="skimage.feature.draw_multiblock_lbp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">draw_multiblock_lbp</span></code></a></p></td>
<td><p>Multi-block local binary pattern visualization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.fisher_vector" title="skimage.feature.fisher_vector"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fisher_vector</span></code></a></p></td>
<td><p>Compute the Fisher vector given some descriptors/vectors, and an associated estimated GMM.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.graycomatrix" title="skimage.feature.graycomatrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">graycomatrix</span></code></a></p></td>
<td><p>Calculate the gray-level co-occurrence matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.graycoprops" title="skimage.feature.graycoprops"><code class="xref py py-obj docutils literal notranslate"><span class="pre">graycoprops</span></code></a></p></td>
<td><p>Calculate texture properties of a GLCM.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.haar_like_feature" title="skimage.feature.haar_like_feature"><code class="xref py py-obj docutils literal notranslate"><span class="pre">haar_like_feature</span></code></a></p></td>
<td><p>Compute the Haar-like features for a region of interest (ROI) of an integral image.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.haar_like_feature_coord" title="skimage.feature.haar_like_feature_coord"><code class="xref py py-obj docutils literal notranslate"><span class="pre">haar_like_feature_coord</span></code></a></p></td>
<td><p>Compute the coordinates of Haar-like features.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.hessian_matrix" title="skimage.feature.hessian_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hessian_matrix</span></code></a></p></td>
<td><p>Compute the Hessian matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.hessian_matrix_det" title="skimage.feature.hessian_matrix_det"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hessian_matrix_det</span></code></a></p></td>
<td><p>Compute the approximate Hessian Determinant over an image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.hessian_matrix_eigvals" title="skimage.feature.hessian_matrix_eigvals"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hessian_matrix_eigvals</span></code></a></p></td>
<td><p>Compute eigenvalues of Hessian matrix.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.hog" title="skimage.feature.hog"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hog</span></code></a></p></td>
<td><p>Extract Histogram of Oriented Gradients (HOG) for a given image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.learn_gmm" title="skimage.feature.learn_gmm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learn_gmm</span></code></a></p></td>
<td><p>Estimate a Gaussian mixture model (GMM) given a set of descriptors and number of modes (i.e. Gaussians).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.local_binary_pattern" title="skimage.feature.local_binary_pattern"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_binary_pattern</span></code></a></p></td>
<td><p>Compute the local binary patterns (LBP) of an image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.match_descriptors" title="skimage.feature.match_descriptors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">match_descriptors</span></code></a></p></td>
<td><p>Brute-force matching of descriptors.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.match_template" title="skimage.feature.match_template"><code class="xref py py-obj docutils literal notranslate"><span class="pre">match_template</span></code></a></p></td>
<td><p>Match a template to a 2-D or 3-D image using normalized correlation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.multiblock_lbp" title="skimage.feature.multiblock_lbp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiblock_lbp</span></code></a></p></td>
<td><p>Multi-block local binary pattern (MB-LBP).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.multiscale_basic_features" title="skimage.feature.multiscale_basic_features"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multiscale_basic_features</span></code></a></p></td>
<td><p>Local features for a single- or multi-channel nd image.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.peak_local_max" title="skimage.feature.peak_local_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">peak_local_max</span></code></a></p></td>
<td><p>Find peaks in an image as coordinate list.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.plot_matched_features" title="skimage.feature.plot_matched_features"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_matched_features</span></code></a></p></td>
<td><p>Plot matched features between two images.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.shape_index" title="skimage.feature.shape_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shape_index</span></code></a></p></td>
<td><p>Compute the shape index.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.structure_tensor" title="skimage.feature.structure_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">structure_tensor</span></code></a></p></td>
<td><p>Compute structure tensor using sum of squared differences.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.structure_tensor_eigenvalues" title="skimage.feature.structure_tensor_eigenvalues"><code class="xref py py-obj docutils literal notranslate"><span class="pre">structure_tensor_eigenvalues</span></code></a></p></td>
<td><p>Compute eigenvalues of structure tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.BRIEF" title="skimage.feature.BRIEF"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BRIEF</span></code></a></p></td>
<td><p>BRIEF binary descriptor extractor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.CENSURE" title="skimage.feature.CENSURE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CENSURE</span></code></a></p></td>
<td><p>CENSURE keypoint detector.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.Cascade" title="skimage.feature.Cascade"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Cascade</span></code></a></p></td>
<td><p>Class for cascade of classifiers that is used for object detection.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#skimage.feature.ORB" title="skimage.feature.ORB"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ORB</span></code></a></p></td>
<td><p>Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#skimage.feature.SIFT" title="skimage.feature.SIFT"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SIFT</span></code></a></p></td>
<td><p>SIFT feature detection and descriptor extraction.</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.blob_dog">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">blob_dog</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overlap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_rel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/blob.py#L221-L409"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.blob_dog" title="Link to this definition">#</a></dt>
<dd><p>Finds blobs in the given grayscale image.</p>
<p>Blobs are found using the Difference of Gaussian (DoG) method <a class="reference internal" href="#rf5218630e229-1" id="id1">[1]</a>, <a class="reference internal" href="#rf5218630e229-2" id="id2">[2]</a>.
For each blob found, the method returns its coordinates and the standard
deviation of the Gaussian kernel that detected the blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input grayscale image, blobs are assumed to be light on dark
background (white on black).</p>
</dd>
<dt><strong>min_sigma</strong><span class="classifier">scalar or sequence of scalars, optional</span></dt><dd><p>The minimum standard deviation for Gaussian kernel. Keep this low to
detect smaller blobs. The standard deviations of the Gaussian filter
are given for each axis as a sequence, or as a single number, in
which case it is equal for all axes.</p>
</dd>
<dt><strong>max_sigma</strong><span class="classifier">scalar or sequence of scalars, optional</span></dt><dd><p>The maximum standard deviation for Gaussian kernel. Keep this high to
detect larger blobs. The standard deviations of the Gaussian filter
are given for each axis as a sequence, or as a single number, in
which case it is equal for all axes.</p>
</dd>
<dt><strong>sigma_ratio</strong><span class="classifier">float, optional</span></dt><dd><p>The ratio between the standard deviation of Gaussian Kernels used for
computing the Difference of Gaussians</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float or None, optional</span></dt><dd><p>The absolute lower bound for scale space maxima. Local maxima smaller
than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> are ignored. Reduce this to detect blobs with lower
intensities. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is also specified, whichever threshold
is larger will be used. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is used instead.</p>
</dd>
<dt><strong>overlap</strong><span class="classifier">float, optional</span></dt><dd><p>A value between 0 and 1. If the area of two blobs overlaps by a
fraction greater than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code>, the smaller blob is eliminated.</p>
</dd>
<dt><strong>threshold_rel</strong><span class="classifier">float or None, optional</span></dt><dd><p>Minimum intensity of peaks, calculated as
<code class="docutils literal notranslate"><span class="pre">max(dog_space)</span> <span class="pre">*</span> <span class="pre">threshold_rel</span></code>, where <code class="docutils literal notranslate"><span class="pre">dog_space</span></code> refers to the
stack of Difference-of-Gaussian (DoG) images computed internally. This
should have a value between 0 and 1. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> is used
instead.</p>
</dd>
<dt><strong>exclude_border</strong><span class="classifier">tuple of ints, int, or False, optional</span></dt><dd><p>If tuple of ints, the length of the tuple must match the input array’s
dimensionality.  Each element of the tuple will exclude peaks from
within <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image along that
dimension.
If nonzero int, <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code> excludes peaks from within
<code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image.
If zero or False, peaks are identified regardless of their
distance from the border.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>A</strong><span class="classifier">(n, image.ndim + sigma) ndarray</span></dt><dd><p>A 2d array with each row representing 2 coordinate values for a 2D
image, or 3 coordinate values for a 3D image, plus the sigma(s) used.
When a single sigma is passed, outputs are:
<code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">c,</span> <span class="pre">sigma)</span></code> or <code class="docutils literal notranslate"><span class="pre">(p,</span> <span class="pre">r,</span> <span class="pre">c,</span> <span class="pre">sigma)</span></code> where <code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">c)</span></code> or
<code class="docutils literal notranslate"><span class="pre">(p,</span> <span class="pre">r,</span> <span class="pre">c)</span></code> are coordinates of the blob and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is the standard
deviation of the Gaussian kernel which detected the blob. When an
anisotropic gaussian is used (sigmas per dimension), the detected sigma
is returned for each dimension.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="skimage.filters.html#skimage.filters.difference_of_gaussians" title="skimage.filters.difference_of_gaussians"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.filters.difference_of_gaussians</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The radius of each blob is approximately <span class="math notranslate nohighlight">\(\sqrt{2}\sigma\)</span> for
a 2-D image and <span class="math notranslate nohighlight">\(\sqrt{3}\sigma\)</span> for a 3-D image.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rf5218630e229-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach">https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach</a></p>
</div>
<div class="citation" id="rf5218630e229-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Lowe, D. G. “Distinctive Image Features from Scale-Invariant
Keypoints.” International Journal of Computer Vision 60, 91–110 (2004).
<a class="reference external" href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a>
<a class="reference external" href="https://doi.org/10.1023/B:VISI.0000029664.99615.94">DOI:10.1023/B:VISI.0000029664.99615.94</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coins</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">coins</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span><span class="o">.</span><span class="n">blob_dog</span><span class="p">(</span><span class="n">coins</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">min_sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_sigma</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="go">array([[128., 155.,  10.],</span>
<span class="go">       [198., 155.,  10.],</span>
<span class="go">       [124., 338.,  10.],</span>
<span class="go">       [127., 102.,  10.],</span>
<span class="go">       [193., 281.,  10.],</span>
<span class="go">       [126., 208.,  10.],</span>
<span class="go">       [267., 115.,  10.],</span>
<span class="go">       [197., 102.,  10.],</span>
<span class="go">       [198., 215.,  10.],</span>
<span class="go">       [123., 279.,  10.],</span>
<span class="go">       [126.,  46.,  10.],</span>
<span class="go">       [259., 247.,  10.],</span>
<span class="go">       [196.,  43.,  10.],</span>
<span class="go">       [ 54., 276.,  10.],</span>
<span class="go">       [267., 358.,  10.],</span>
<span class="go">       [ 58., 100.,  10.],</span>
<span class="go">       [259., 305.,  10.],</span>
<span class="go">       [185., 347.,  16.],</span>
<span class="go">       [261., 174.,  16.],</span>
<span class="go">       [ 46., 336.,  16.],</span>
<span class="go">       [ 54., 217.,  10.],</span>
<span class="go">       [ 55., 157.,  10.],</span>
<span class="go">       [ 57.,  41.,  10.],</span>
<span class="go">       [260.,  47.,  16.]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Blobs are bright on dark or dark on bright regions in an image. In this example, blobs are detected using 3 algorithms. The image used in this case is the Hubble eXtreme Deep Field. Each bright dot in the image is a star or a galaxy."><img alt="" src="../_images/sphx_glr_plot_blob_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_blob.html#sphx-glr-auto-examples-features-detection-plot-blob-py"><span class="std std-ref">Blob Detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Blob Detection</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.blob_doh">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">blob_doh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overlap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_rel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/blob.py#L584-L715"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.blob_doh" title="Link to this definition">#</a></dt>
<dd><p>Finds blobs in the given grayscale image.</p>
<p>Blobs are found using the Determinant of Hessian method <a class="reference internal" href="#ra19a7aed16ca-1" id="id5">[1]</a>. For each blob
found, the method returns its coordinates and the standard deviation
of the Gaussian Kernel used for the Hessian matrix whose determinant
detected the blob. Determinant of Hessians is approximated using <a class="reference internal" href="#ra19a7aed16ca-2" id="id6">[2]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D ndarray</span></dt><dd><p>Input grayscale image.Blobs can either be light on dark or vice versa.</p>
</dd>
<dt><strong>min_sigma</strong><span class="classifier">float, optional</span></dt><dd><p>The minimum standard deviation for Gaussian Kernel used to compute
Hessian matrix. Keep this low to detect smaller blobs.</p>
</dd>
<dt><strong>max_sigma</strong><span class="classifier">float, optional</span></dt><dd><p>The maximum standard deviation for Gaussian Kernel used to compute
Hessian matrix. Keep this high to detect larger blobs.</p>
</dd>
<dt><strong>num_sigma</strong><span class="classifier">int, optional</span></dt><dd><p>The number of intermediate values of standard deviations to consider
between <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_sigma</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_sigma</span></code>.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float or None, optional</span></dt><dd><p>The absolute lower bound for scale space maxima. Local maxima smaller
than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> are ignored. Reduce this to detect blobs with lower
intensities. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is also specified, whichever threshold
is larger will be used. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is used instead.</p>
</dd>
<dt><strong>overlap</strong><span class="classifier">float, optional</span></dt><dd><p>A value between 0 and 1. If the area of two blobs overlaps by a
fraction greater than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code>, the smaller blob is eliminated.</p>
</dd>
<dt><strong>log_scale</strong><span class="classifier">bool, optional</span></dt><dd><p>If set intermediate values of standard deviations are interpolated
using a logarithmic scale to the base <code class="xref py py-obj docutils literal notranslate"><span class="pre">10</span></code>. If not, linear
interpolation is used.</p>
</dd>
<dt><strong>threshold_rel</strong><span class="classifier">float or None, optional</span></dt><dd><p>Minimum intensity of peaks, calculated as
<code class="docutils literal notranslate"><span class="pre">max(doh_space)</span> <span class="pre">*</span> <span class="pre">threshold_rel</span></code>, where <code class="docutils literal notranslate"><span class="pre">doh_space</span></code> refers to the
stack of Determinant-of-Hessian (DoH) images computed internally. This
should have a value between 0 and 1. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> is used
instead.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>A</strong><span class="classifier">(n, 3) ndarray</span></dt><dd><p>A 2d array with each row representing 3 values, <code class="docutils literal notranslate"><span class="pre">(y,x,sigma)</span></code>
where <code class="docutils literal notranslate"><span class="pre">(y,x)</span></code> are coordinates of the blob and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is the
standard deviation of the Gaussian kernel of the Hessian Matrix whose
determinant detected the blob.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The radius of each blob is approximately <code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma</span></code>.
Computation of Determinant of Hessians is independent of the standard
deviation. Therefore detecting larger blobs won’t take more time. In
methods line <a class="reference internal" href="#skimage.feature.blob_dog" title="skimage.feature.blob_dog"><code class="xref py py-meth docutils literal notranslate"><span class="pre">blob_dog()</span></code></a> and <a class="reference internal" href="#skimage.feature.blob_log" title="skimage.feature.blob_log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">blob_log()</span></code></a> the computation
of Gaussians for larger <code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma</span></code> takes more time. The downside is that
this method can’t be used for detecting blobs of radius less than <code class="xref py py-obj docutils literal notranslate"><span class="pre">3px</span></code>
due to the box filters used in the approximation of Hessian Determinant.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra19a7aed16ca-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian">https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian</a></p>
</div>
<div class="citation" id="ra19a7aed16ca-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool,
“SURF: Speeded Up Robust Features”
<a class="reference external" href="ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf">ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">coins</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span><span class="o">.</span><span class="n">blob_doh</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="go">array([[197.        , 153.        ,  20.33333333],</span>
<span class="go">       [124.        , 336.        ,  20.33333333],</span>
<span class="go">       [126.        , 153.        ,  20.33333333],</span>
<span class="go">       [195.        , 100.        ,  23.55555556],</span>
<span class="go">       [192.        , 212.        ,  23.55555556],</span>
<span class="go">       [121.        , 271.        ,  30.        ],</span>
<span class="go">       [126.        , 101.        ,  20.33333333],</span>
<span class="go">       [193.        , 275.        ,  23.55555556],</span>
<span class="go">       [123.        , 205.        ,  20.33333333],</span>
<span class="go">       [270.        , 363.        ,  30.        ],</span>
<span class="go">       [265.        , 113.        ,  23.55555556],</span>
<span class="go">       [262.        , 243.        ,  23.55555556],</span>
<span class="go">       [185.        , 348.        ,  30.        ],</span>
<span class="go">       [156.        , 302.        ,  30.        ],</span>
<span class="go">       [123.        ,  44.        ,  23.55555556],</span>
<span class="go">       [260.        , 173.        ,  30.        ],</span>
<span class="go">       [197.        ,  44.        ,  20.33333333]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Blobs are bright on dark or dark on bright regions in an image. In this example, blobs are detected using 3 algorithms. The image used in this case is the Hubble eXtreme Deep Field. Each bright dot in the image is a star or a galaxy."><img alt="" src="../_images/sphx_glr_plot_blob_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_blob.html#sphx-glr-auto-examples-features-detection-plot-blob-py"><span class="std std-ref">Blob Detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Blob Detection</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.blob_log">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">blob_log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overlap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_rel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/blob.py#L412-L581"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.blob_log" title="Link to this definition">#</a></dt>
<dd><p>Finds blobs in the given grayscale image.</p>
<p>Blobs are found using the Laplacian of Gaussian (LoG) method <a class="reference internal" href="#r520e53dd5fa2-1" id="id9">[1]</a>.
For each blob found, the method returns its coordinates and the standard
deviation of the Gaussian kernel that detected the blob.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input grayscale image, blobs are assumed to be light on dark
background (white on black).</p>
</dd>
<dt><strong>min_sigma</strong><span class="classifier">scalar or sequence of scalars, optional</span></dt><dd><p>the minimum standard deviation for Gaussian kernel. Keep this low to
detect smaller blobs. The standard deviations of the Gaussian filter
are given for each axis as a sequence, or as a single number, in
which case it is equal for all axes.</p>
</dd>
<dt><strong>max_sigma</strong><span class="classifier">scalar or sequence of scalars, optional</span></dt><dd><p>The maximum standard deviation for Gaussian kernel. Keep this high to
detect larger blobs. The standard deviations of the Gaussian filter
are given for each axis as a sequence, or as a single number, in
which case it is equal for all axes.</p>
</dd>
<dt><strong>num_sigma</strong><span class="classifier">int, optional</span></dt><dd><p>The number of intermediate values of standard deviations to consider
between <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_sigma</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">max_sigma</span></code>.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float or None, optional</span></dt><dd><p>The absolute lower bound for scale space maxima. Local maxima smaller
than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> are ignored. Reduce this to detect blobs with lower
intensities. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is also specified, whichever threshold
is larger will be used. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> is used instead.</p>
</dd>
<dt><strong>overlap</strong><span class="classifier">float, optional</span></dt><dd><p>A value between 0 and 1. If the area of two blobs overlaps by a
fraction greater than <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code>, the smaller blob is eliminated.</p>
</dd>
<dt><strong>log_scale</strong><span class="classifier">bool, optional</span></dt><dd><p>If set intermediate values of standard deviations are interpolated
using a logarithmic scale to the base <code class="xref py py-obj docutils literal notranslate"><span class="pre">10</span></code>. If not, linear
interpolation is used.</p>
</dd>
<dt><strong>threshold_rel</strong><span class="classifier">float or None, optional</span></dt><dd><p>Minimum intensity of peaks, calculated as
<code class="docutils literal notranslate"><span class="pre">max(log_space)</span> <span class="pre">*</span> <span class="pre">threshold_rel</span></code>, where <code class="docutils literal notranslate"><span class="pre">log_space</span></code> refers to the
stack of Laplacian-of-Gaussian (LoG) images computed internally. This
should have a value between 0 and 1. If None, <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code> is used
instead.</p>
</dd>
<dt><strong>exclude_border</strong><span class="classifier">tuple of ints, int, or False, optional</span></dt><dd><p>If tuple of ints, the length of the tuple must match the input array’s
dimensionality.  Each element of the tuple will exclude peaks from
within <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image along that
dimension.
If nonzero int, <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code> excludes peaks from within
<code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image.
If zero or False, peaks are identified regardless of their
distance from the border.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>A</strong><span class="classifier">(n, image.ndim + sigma) ndarray</span></dt><dd><p>A 2d array with each row representing 2 coordinate values for a 2D
image, or 3 coordinate values for a 3D image, plus the sigma(s) used.
When a single sigma is passed, outputs are:
<code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">c,</span> <span class="pre">sigma)</span></code> or <code class="docutils literal notranslate"><span class="pre">(p,</span> <span class="pre">r,</span> <span class="pre">c,</span> <span class="pre">sigma)</span></code> where <code class="docutils literal notranslate"><span class="pre">(r,</span> <span class="pre">c)</span></code> or
<code class="docutils literal notranslate"><span class="pre">(p,</span> <span class="pre">r,</span> <span class="pre">c)</span></code> are coordinates of the blob and <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is the standard
deviation of the Gaussian kernel which detected the blob. When an
anisotropic gaussian is used (sigmas per dimension), the detected sigma
is returned for each dimension.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The radius of each blob is approximately <span class="math notranslate nohighlight">\(\sqrt{2}\sigma\)</span> for
a 2-D image and <span class="math notranslate nohighlight">\(\sqrt{3}\sigma\)</span> for a 3-D image.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r520e53dd5fa2-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian">https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="n">exposure</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">coins</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">exposure</span><span class="o">.</span><span class="n">equalize_hist</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># improves detection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span><span class="o">.</span><span class="n">blob_log</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">.3</span><span class="p">)</span>
<span class="go">array([[124.        , 336.        ,  11.88888889],</span>
<span class="go">       [198.        , 155.        ,  11.88888889],</span>
<span class="go">       [194.        , 213.        ,  17.33333333],</span>
<span class="go">       [121.        , 272.        ,  17.33333333],</span>
<span class="go">       [263.        , 244.        ,  17.33333333],</span>
<span class="go">       [194.        , 276.        ,  17.33333333],</span>
<span class="go">       [266.        , 115.        ,  11.88888889],</span>
<span class="go">       [128.        , 154.        ,  11.88888889],</span>
<span class="go">       [260.        , 174.        ,  17.33333333],</span>
<span class="go">       [198.        , 103.        ,  11.88888889],</span>
<span class="go">       [126.        , 208.        ,  11.88888889],</span>
<span class="go">       [127.        , 102.        ,  11.88888889],</span>
<span class="go">       [263.        , 302.        ,  17.33333333],</span>
<span class="go">       [197.        ,  44.        ,  11.88888889],</span>
<span class="go">       [185.        , 344.        ,  17.33333333],</span>
<span class="go">       [126.        ,  46.        ,  11.88888889],</span>
<span class="go">       [113.        , 323.        ,   1.        ]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Blobs are bright on dark or dark on bright regions in an image. In this example, blobs are detected using 3 algorithms. The image used in this case is the Hubble eXtreme Deep Field. Each bright dot in the image is a star or a galaxy."><img alt="" src="../_images/sphx_glr_plot_blob_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_blob.html#sphx-glr-auto-examples-features-detection-plot-blob-py"><span class="std std-ref">Blob Detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Blob Detection</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.canny">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">canny</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">low_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">high_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_quantiles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_canny.py#L103-L262"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.canny" title="Link to this definition">#</a></dt>
<dd><p>Edge filter an image using the Canny algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Grayscale input image to detect edges on; can be of any dtype.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation of the Gaussian filter.</p>
</dd>
<dt><strong>low_threshold</strong><span class="classifier">float, optional</span></dt><dd><p>Lower bound for hysteresis thresholding (linking edges).
If None, low_threshold is set to 10% of dtype’s max.</p>
</dd>
<dt><strong>high_threshold</strong><span class="classifier">float, optional</span></dt><dd><p>Upper bound for hysteresis thresholding (linking edges).
If None, high_threshold is set to 20% of dtype’s max.</p>
</dd>
<dt><strong>mask</strong><span class="classifier">array, dtype=bool, optional</span></dt><dd><p>Mask to limit the application of Canny to a certain area.</p>
</dd>
<dt><strong>use_quantiles</strong><span class="classifier">bool, optional</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> then treat low_threshold and high_threshold as
quantiles of the edge magnitude image, rather than absolute
edge magnitude values. If <code class="docutils literal notranslate"><span class="pre">True</span></code> then the thresholds must be
in the range [0, 1].</p>
</dd>
<dt><strong>mode</strong><span class="classifier">str, {‘reflect’, ‘constant’, ‘nearest’, ‘mirror’, ‘wrap’}</span></dt><dd><p>The <code class="docutils literal notranslate"><span class="pre">mode</span></code> parameter determines how the array borders are
handled during Gaussian filtering, where <code class="docutils literal notranslate"><span class="pre">cval</span></code> is the value when
mode is equal to ‘constant’.</p>
</dd>
<dt><strong>cval</strong><span class="classifier">float, optional</span></dt><dd><p>Value to fill past edges of input if <code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code> is ‘constant’.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">2D array (image)</span></dt><dd><p>The binary edge map.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="skimage.filters.html#skimage.filters.sobel" title="skimage.filters.sobel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.filters.sobel</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The steps of the algorithm are as follows:</p>
<ul class="simple">
<li><p>Smooth the image using a Gaussian with <code class="docutils literal notranslate"><span class="pre">sigma</span></code> width.</p></li>
<li><p>Apply the horizontal and vertical Sobel operators to get the gradients
within the image. The edge strength is the norm of the gradient.</p></li>
<li><p>Thin potential edges to 1-pixel wide curves. First, find the normal
to the edge at each point. This is done by looking at the
signs and the relative magnitude of the X-Sobel and Y-Sobel
to sort the points into 4 categories: horizontal, vertical,
diagonal and antidiagonal. Then look in the normal and reverse
directions to see if the values in either of those directions are
greater than the point in question. Use interpolation to get a mix of
points instead of picking the one that’s the closest to the normal.</p></li>
<li><p>Perform a hysteresis thresholding: first label all points above the
high threshold as edges. Then recursively label any point above the
low threshold that is 8-connected to a labeled point as an edge.</p></li>
</ul>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r5f5bcbc11495-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Canny, J., A Computational Approach To Edge Detection, IEEE Trans.
Pattern Analysis and Machine Intelligence, 8:679-714, 1986
<a class="reference external" href="https://doi.org/10.1109/TPAMI.1986.4767851">DOI:10.1109/TPAMI.1986.4767851</a></p>
</div>
<div class="citation" id="r5f5bcbc11495-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>William Green’s Canny tutorial
<a class="reference external" href="https://en.wikipedia.org/wiki/Canny_edge_detector">https://en.wikipedia.org/wiki/Canny_edge_detector</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage</span> <span class="kn">import</span> <span class="n">feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Generate noisy image of a square</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">im</span><span class="p">[</span><span class="mi">64</span><span class="p">:</span><span class="o">-</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">:</span><span class="o">-</span><span class="mi">64</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">im</span> <span class="o">+=</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># First trial with the Canny filter, with the default smoothing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edges1</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">canny</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Increase the smoothing for better results</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edges2</span> <span class="o">=</span> <span class="n">feature</span><span class="o">.</span><span class="n">canny</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients.The Gaussian reduces the effect of noise present in the image. Then, potential edges are thinned down to 1-pixel curves by removing non-maximum pixels of the gradient magnitude. Finally, edge pixels are kept or removed using hysteresis thresholding on the gradient magnitude."><img alt="" src="../_images/sphx_glr_plot_canny_thumb.png" />
<p><a class="reference internal" href="../auto_examples/edges/plot_canny.html#sphx-glr-auto-examples-edges-plot-canny-py"><span class="std std-ref">Canny edge detector</span></a></p>
  <div class="sphx-glr-thumbnail-title">Canny edge detector</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The Hough transform in its simplest form is a method to detect straight lines [1]_."><img alt="" src="../_images/sphx_glr_plot_line_hough_transform_thumb.png" />
<p><a class="reference internal" href="../auto_examples/edges/plot_line_hough_transform.html#sphx-glr-auto-examples-edges-plot-line-hough-transform-py"><span class="std std-ref">Straight line Hough transform</span></a></p>
  <div class="sphx-glr-thumbnail-title">Straight line Hough transform</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The Hough transform in its simplest form is a method to detect straight lines but it can also be used to detect circles or ellipses. The algorithm assumes that the edge is detected and it is robust against noise or missing points."><img alt="" src="../_images/sphx_glr_plot_circular_elliptical_hough_transform_thumb.png" />
<p><a class="reference internal" href="../auto_examples/edges/plot_circular_elliptical_hough_transform.html#sphx-glr-auto-examples-edges-plot-circular-elliptical-hough-transform-py"><span class="std std-ref">Circular and Elliptical Hough Transforms</span></a></p>
  <div class="sphx-glr-thumbnail-title">Circular and Elliptical Hough Transforms</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="When trying out different segmentation methods, how do you know which one is best? If you have a ground truth or gold standard segmentation, you can use various metrics to check how close each automated method comes to the truth. In this example we use an easy-to-segment image as an example of how to interpret various segmentation metrics. We will use the adapted Rand error and the variation of information as example metrics, and see how oversegmentation (splitting of true segments into too many sub-segments) and undersegmentation (merging of different true segments into a single segment) affect the different scores."><img alt="" src="../_images/sphx_glr_plot_metrics_thumb.png" />
<p><a class="reference internal" href="../auto_examples/segmentation/plot_metrics.html#sphx-glr-auto-examples-segmentation-plot-metrics-py"><span class="std std-ref">Evaluating segmentation metrics</span></a></p>
  <div class="sphx-glr-thumbnail-title">Evaluating segmentation metrics</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we will see how to segment objects from a background. We use the coins image from skimage.data, which shows several coins outlined against a darker background."><img alt="" src="../_images/sphx_glr_plot_coins_segmentation_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_coins_segmentation.html#sphx-glr-auto-examples-applications-plot-coins-segmentation-py"><span class="std std-ref">Comparing edge-based and region-based segmentation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Comparing edge-based and region-based segmentation</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_fast">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_fast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L880-L942"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_fast" title="Link to this definition">#</a></dt>
<dd><p>Extract FAST corners for a given image.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>n</strong><span class="classifier">int, optional</span></dt><dd><p>Minimum number of consecutive pixels out of 16 pixels on the circle
that should all be either brighter or darker w.r.t testpixel.
A point c on the circle is darker w.r.t test pixel p if
<code class="xref py py-obj docutils literal notranslate"><span class="pre">Ic</span> <span class="pre">&lt;</span> <span class="pre">Ip</span> <span class="pre">-</span> <span class="pre">threshold</span></code> and brighter if <code class="xref py py-obj docutils literal notranslate"><span class="pre">Ic</span> <span class="pre">&gt;</span> <span class="pre">Ip</span> <span class="pre">+</span> <span class="pre">threshold</span></code>. Also
stands for the n in <code class="xref py py-obj docutils literal notranslate"><span class="pre">FAST-n</span></code> corner detector.</p>
</dd>
<dt><strong>threshold</strong><span class="classifier">float, optional</span></dt><dd><p>Threshold used in deciding whether the pixels on the circle are
brighter, darker or similar w.r.t. the test pixel. Decrease the
threshold when more corners are desired and vice-versa.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>response</strong><span class="classifier">ndarray</span></dt><dd><p>FAST corner response image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r92fff83e7342-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Rosten, E., &amp; Drummond, T. (2006, May). Machine learning for
high-speed corner detection. In European conference on computer
vision (pp. 430-443). Springer, Berlin, Heidelberg.
<a class="reference external" href="https://doi.org/10.1007/11744023_34">DOI:10.1007/11744023_34</a>
<a class="reference external" href="http://www.edwardrosten.com/work/rosten_2006_machine.pdf">http://www.edwardrosten.com/work/rosten_2006_machine.pdf</a></p>
</div>
<div class="citation" id="r92fff83e7342-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Wikipedia, “Features from accelerated segment test”,
<a class="reference external" href="https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test">https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_fast</span><span class="p">,</span> <span class="n">corner_peaks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_fast</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[3, 3],</span>
<span class="go">       [3, 8],</span>
<span class="go">       [8, 3],</span>
<span class="go">       [8, 8]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_foerstner">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_foerstner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L796-L877"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_foerstner" title="Link to this definition">#</a></dt>
<dd><p>Compute Foerstner corner measure response image.</p>
<p>This corner detector uses information from the auto-correlation matrix A:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="p">[(</span><span class="n">imx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>   <span class="p">(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axx</span> <span class="n">Axy</span><span class="p">]</span>
    <span class="p">[(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)</span>   <span class="p">(</span><span class="n">imy</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>   <span class="p">[</span><span class="n">Axy</span> <span class="n">Ayy</span><span class="p">]</span>
</pre></div>
</div>
<p>Where imx and imy are first derivatives, averaged with a gaussian filter.
The corner measure is then defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">/</span> <span class="n">trace</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>           <span class="p">(</span><span class="n">size</span> <span class="n">of</span> <span class="n">error</span> <span class="n">ellipse</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">/</span> <span class="n">trace</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>    <span class="p">(</span><span class="n">roundness</span> <span class="n">of</span> <span class="n">error</span> <span class="n">ellipse</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used as
weighting function for the auto-correlation matrix.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>w</strong><span class="classifier">ndarray</span></dt><dd><p>Error ellipse sizes.</p>
</dd>
<dt><strong>q</strong><span class="classifier">ndarray</span></dt><dd><p>Roundness of error ellipse.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r9d429c80f73f-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Förstner, W., &amp; Gülch, E. (1987, June). A fast operator for
detection and precise location of distinct points, corners and
centres of circular features. In Proc. ISPRS intercommission
conference on fast processing of photogrammetric data (pp. 281-305).
<a class="reference external" href="https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf">https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf</a></p>
</div>
<div class="citation" id="r9d429c80f73f-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Corner_detection">https://en.wikipedia.org/wiki/Corner_detection</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_foerstner</span><span class="p">,</span> <span class="n">corner_peaks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">corner_foerstner</span><span class="p">(</span><span class="n">square</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_thresh</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roundness_thresh</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">foerstner</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">&gt;</span> <span class="n">roundness_thresh</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span> <span class="o">&gt;</span> <span class="n">accuracy_thresh</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_peaks</span><span class="p">(</span><span class="n">foerstner</span><span class="p">,</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 7],</span>
<span class="go">       [7, 2],</span>
<span class="go">       [7, 7]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_harris">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_harris</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L654-L731"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_harris" title="Link to this definition">#</a></dt>
<dd><p>Compute Harris corner measure response image.</p>
<p>This corner detector uses information from the auto-correlation matrix A:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="p">[(</span><span class="n">imx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>   <span class="p">(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axx</span> <span class="n">Axy</span><span class="p">]</span>
    <span class="p">[(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)</span>   <span class="p">(</span><span class="n">imy</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>   <span class="p">[</span><span class="n">Axy</span> <span class="n">Ayy</span><span class="p">]</span>
</pre></div>
</div>
<p>Where imx and imy are first derivatives, averaged with a gaussian filter.
The corner measure is then defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">-</span> <span class="n">k</span> <span class="o">*</span> <span class="n">trace</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>or:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span> <span class="o">*</span> <span class="n">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">trace</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>method</strong><span class="classifier">{‘k’, ‘eps’}, optional</span></dt><dd><p>Method to compute the response image from the auto-correlation matrix.</p>
</dd>
<dt><strong>k</strong><span class="classifier">float, optional</span></dt><dd><p>Sensitivity factor to separate corners from edges, typically in range
<code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0.2]</span></code>. Small values of k result in detection of sharp corners.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float, optional</span></dt><dd><p>Normalisation factor (Noble’s corner measure).</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used as
weighting function for the auto-correlation matrix.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>response</strong><span class="classifier">ndarray</span></dt><dd><p>Harris response image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc19c7843bb6d-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Corner_detection">https://en.wikipedia.org/wiki/Corner_detection</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_harris</span><span class="p">,</span> <span class="n">corner_peaks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">square</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 7],</span>
<span class="go">       [7, 2],</span>
<span class="go">       [7, 7]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this simplified example we first generate two synthetic images as if they were taken from different view points."><img alt="" src="../_images/sphx_glr_plot_matching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_matching.html#sphx-glr-auto-examples-transform-plot-matching-py"><span class="std std-ref">Robust matching using RANSAC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust matching using RANSAC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how a set of images can be assembled under the hypothesis of rigid body motions."><img alt="" src="../_images/sphx_glr_plot_stitching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/registration/plot_stitching.html#sphx-glr-auto-examples-registration-plot-stitching-py"><span class="std std-ref">Assemble images with simple image stitching</span></a></p>
  <div class="sphx-glr-thumbnail-title">Assemble images with simple image stitching</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Detect corner points using the Harris corner detector and determine the subpixel position of corners ([1]_, [2]_)."><img alt="" src="../_images/sphx_glr_plot_corner_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py"><span class="std std-ref">Corner detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Corner detection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the BRIEF binary description algorithm. The descriptor consists of relatively few bits and can be computed using a set of intensity difference tests. The short binary descriptor results in low memory footprint and very efficient matching based on the Hamming distance metric. BRIEF does not provide rotation-invariance. Scale-invariance can be achieved by detecting and extracting features at different scales."><img alt="" src="../_images/sphx_glr_plot_brief_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_brief.html#sphx-glr-auto-examples-features-detection-plot-brief-py"><span class="std std-ref">BRIEF binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">BRIEF binary descriptor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_kitchen_rosenfeld">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_kitchen_rosenfeld</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L603-L651"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_kitchen_rosenfeld" title="Link to this definition">#</a></dt>
<dd><p>Compute Kitchen and Rosenfeld corner measure response image.</p>
<p>The corner measure is calculated as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">imxx</span> <span class="o">*</span> <span class="n">imy</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">imyy</span> <span class="o">*</span> <span class="n">imx</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">imxy</span> <span class="o">*</span> <span class="n">imx</span> <span class="o">*</span> <span class="n">imy</span><span class="p">)</span>
    <span class="o">/</span> <span class="p">(</span><span class="n">imx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">imy</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Where imx and imy are the first and imxx, imxy, imyy the second
derivatives.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">{‘constant’, ‘reflect’, ‘wrap’, ‘nearest’, ‘mirror’}, optional</span></dt><dd><p>How to handle values outside the image borders.</p>
</dd>
<dt><strong>cval</strong><span class="classifier">float, optional</span></dt><dd><p>Used in conjunction with mode ‘constant’, the value outside
the image boundaries.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>response</strong><span class="classifier">ndarray</span></dt><dd><p>Kitchen and Rosenfeld response image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rd1dae2ae49ff-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Kitchen, L., &amp; Rosenfeld, A. (1982). Gray-level corner detection.
Pattern recognition letters, 1(2), 95-102.
<a class="reference external" href="https://doi.org/10.1016/0167-8655(82)90020-4">DOI:10.1016/0167-8655(82)90020-4</a></p>
</div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_moravec">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_moravec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L1241-L1288"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_moravec" title="Link to this definition">#</a></dt>
<dd><p>Compute Moravec corner measure response image.</p>
<p>This is one of the simplest corner detectors and is comparatively fast but
has several limitations (e.g. not rotation invariant).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>window_size</strong><span class="classifier">int, optional</span></dt><dd><p>Window size.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>response</strong><span class="classifier">ndarray</span></dt><dd><p>Moravec response image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rdf16f0a1a068-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Corner_detection">https://en.wikipedia.org/wiki/Corner_detection</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_moravec</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_moravec</span><span class="p">(</span><span class="n">square</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 2, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_orientations">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_orientations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corners</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L1291-L1355"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_orientations" title="Link to this definition">#</a></dt>
<dd><p>Compute the orientation of corners.</p>
<p>The orientation of corners is computed using the first order central moment
i.e. the center of mass approach. The corner orientation is the angle of
the vector from the corner coordinate to the intensity centroid in the
local neighborhood around the corner calculated using first order central
moment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) array</span></dt><dd><p>Input grayscale image.</p>
</dd>
<dt><strong>corners</strong><span class="classifier">(K, 2) array</span></dt><dd><p>Corner coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>mask</strong><span class="classifier">2D array</span></dt><dd><p>Mask defining the local neighborhood of the corner used for the
calculation of the central moment.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>orientations</strong><span class="classifier">(K, 1) array</span></dt><dd><p>Orientations of corners in the range [-pi, pi].</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r0592d7afdba5-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski
“ORB : An efficient alternative to SIFT and SURF”
<a class="reference external" href="http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf">http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf</a></p>
</div>
<div class="citation" id="r0592d7afdba5-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Paul L. Rosin, “Measuring Corner Properties”
<a class="reference external" href="http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf">http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.morphology</span> <span class="kn">import</span> <span class="n">octagon</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="p">(</span><span class="n">corner_fast</span><span class="p">,</span> <span class="n">corner_peaks</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">corner_orientations</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corners</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_fast</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corners</span>
<span class="go">array([[3, 3],</span>
<span class="go">       [3, 8],</span>
<span class="go">       [8, 3],</span>
<span class="go">       [8, 8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">orientations</span> <span class="o">=</span> <span class="n">corner_orientations</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">corners</span><span class="p">,</span> <span class="n">octagon</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">rad2deg</span><span class="p">(</span><span class="n">orientations</span><span class="p">)</span>
<span class="go">array([  45.,  135.,  -45., -135.])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_peaks">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_peaks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_abs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_rel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_peaks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">footprint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_peaks_per_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L1126-L1238"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_peaks" title="Link to this definition">#</a></dt>
<dd><p>Find peaks in corner measure response image.</p>
<p>This differs from <a class="reference internal" href="#skimage.feature.peak_local_max" title="skimage.feature.peak_local_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.peak_local_max</span></code></a> in that it suppresses
multiple connected peaks with the same accumulator value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>min_distance</strong><span class="classifier">int, optional</span></dt><dd><p>The minimal allowed distance separating peaks.</p>
</dd>
<dt><strong>*</strong><span class="classifier">*</span></dt><dd><p>See <a class="reference internal" href="#skimage.feature.peak_local_max" title="skimage.feature.peak_local_max"><code class="xref py py-meth docutils literal notranslate"><span class="pre">skimage.feature.peak_local_max()</span></code></a>.</p>
</dd>
<dt><strong>p_norm</strong><span class="classifier">float</span></dt><dd><p>Which Minkowski p-norm to use. Should be in the range [1, inf].
A finite large p may cause a ValueError if overflow can occur.
<code class="docutils literal notranslate"><span class="pre">inf</span></code> corresponds to the Chebyshev distance and 2 to the
Euclidean distance.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">ndarray or ndarray of bools</span></dt><dd><ul class="simple">
<li><p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">indices</span> <span class="pre">=</span> <span class="pre">True</span></code>  : (row, column, …) coordinates of peaks.</p></li>
<li><p>If <code class="xref py py-obj docutils literal notranslate"><span class="pre">indices</span> <span class="pre">=</span> <span class="pre">False</span></code> : Boolean array shaped like <code class="xref py py-obj docutils literal notranslate"><span class="pre">image</span></code>, with peaks
represented by True values.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#skimage.feature.peak_local_max" title="skimage.feature.peak_local_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.peak_local_max</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>The default value of <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> has changed to None, which
corresponds to letting <a class="reference internal" href="#skimage.feature.peak_local_max" title="skimage.feature.peak_local_max"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.peak_local_max</span></code></a> decide on the
default. This is equivalent to <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel=0</span></code>.</p>
</div>
<p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_peaks</span></code> limit is applied before suppression of connected peaks.
To limit the number of peaks after suppression, set <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_peaks=np.inf</span></code> and
post-process the output of this function.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">peak_local_max</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span>
<span class="go">array([[0., 0., 0., 0., 0.],</span>
<span class="go">       [0., 0., 0., 0., 0.],</span>
<span class="go">       [0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">peak_local_max</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [3, 2],</span>
<span class="go">       [3, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_peaks</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">array([[2, 2]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this simplified example we first generate two synthetic images as if they were taken from different view points."><img alt="" src="../_images/sphx_glr_plot_matching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_matching.html#sphx-glr-auto-examples-transform-plot-matching-py"><span class="std std-ref">Robust matching using RANSAC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust matching using RANSAC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how a set of images can be assembled under the hypothesis of rigid body motions."><img alt="" src="../_images/sphx_glr_plot_stitching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/registration/plot_stitching.html#sphx-glr-auto-examples-registration-plot-stitching-py"><span class="std std-ref">Assemble images with simple image stitching</span></a></p>
  <div class="sphx-glr-thumbnail-title">Assemble images with simple image stitching</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Detect corner points using the Harris corner detector and determine the subpixel position of corners ([1]_, [2]_)."><img alt="" src="../_images/sphx_glr_plot_corner_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py"><span class="std std-ref">Corner detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Corner detection</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the BRIEF binary description algorithm. The descriptor consists of relatively few bits and can be computed using a set of intensity difference tests. The short binary descriptor results in low memory footprint and very efficient matching based on the Hamming distance metric. BRIEF does not provide rotation-invariance. Scale-invariance can be achieved by detecting and extracting features at different scales."><img alt="" src="../_images/sphx_glr_plot_brief_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_brief.html#sphx-glr-auto-examples-features-detection-plot-brief-py"><span class="std std-ref">BRIEF binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">BRIEF binary descriptor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_shi_tomasi">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_shi_tomasi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L734-L793"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_shi_tomasi" title="Link to this definition">#</a></dt>
<dd><p>Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.</p>
<p>This corner detector uses information from the auto-correlation matrix A:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="p">[(</span><span class="n">imx</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>   <span class="p">(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="n">Axx</span> <span class="n">Axy</span><span class="p">]</span>
    <span class="p">[(</span><span class="n">imx</span><span class="o">*</span><span class="n">imy</span><span class="p">)</span>   <span class="p">(</span><span class="n">imy</span><span class="o">**</span><span class="mi">2</span><span class="p">)]</span>   <span class="p">[</span><span class="n">Axy</span> <span class="n">Ayy</span><span class="p">]</span>
</pre></div>
</div>
<p>Where imx and imy are first derivatives, averaged with a gaussian filter.
The corner measure is then defined as the smaller eigenvalue of A:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">((</span><span class="n">Axx</span> <span class="o">+</span> <span class="n">Ayy</span><span class="p">)</span> <span class="o">-</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">Axx</span> <span class="o">-</span> <span class="n">Ayy</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">Axy</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used as
weighting function for the auto-correlation matrix.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>response</strong><span class="classifier">ndarray</span></dt><dd><p>Shi-Tomasi response image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r26d4c89afc0d-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Corner_detection">https://en.wikipedia.org/wiki/Corner_detection</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_shi_tomasi</span><span class="p">,</span> <span class="n">corner_peaks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_shi_tomasi</span><span class="p">(</span><span class="n">square</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 7],</span>
<span class="go">       [7, 2],</span>
<span class="go">       [7, 7]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.corner_subpix">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">corner_subpix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corners</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">11</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L945-L1123"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.corner_subpix" title="Link to this definition">#</a></dt>
<dd><p>Determine subpixel position of corners.</p>
<p>A statistical test decides whether the corner is defined as the
intersection of two edges or a single peak. Depending on the classification
result, the subpixel corner location is determined based on the local
covariance of the grey-values. If the significance level for either
statistical test is not sufficient, the corner cannot be classified, and
the output subpixel position is set to NaN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>corners</strong><span class="classifier">(K, 2) ndarray</span></dt><dd><p>Corner coordinates <code class="xref py py-obj docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>window_size</strong><span class="classifier">int, optional</span></dt><dd><p>Search window size for subpixel estimation.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, optional</span></dt><dd><p>Significance level for corner classification.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>positions</strong><span class="classifier">(K, 2) ndarray</span></dt><dd><p>Subpixel corner positions. NaN for “not classified” corners.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra33874b5943a-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Förstner, W., &amp; Gülch, E. (1987, June). A fast operator for
detection and precise location of distinct points, corners and
centres of circular features. In Proc. ISPRS intercommission
conference on fast processing of photogrammetric data (pp. 281-305).
<a class="reference external" href="https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf">https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf</a></p>
</div>
<div class="citation" id="ra33874b5943a-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Corner_detection">https://en.wikipedia.org/wiki/Corner_detection</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">corner_harris</span><span class="p">,</span> <span class="n">corner_peaks</span><span class="p">,</span> <span class="n">corner_subpix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="p">[</span><span class="mi">5</span><span class="p">:,</span> <span class="mi">5</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</span>
<span class="go">       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</span>
<span class="go">       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</span>
<span class="go">       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</span>
<span class="go">       [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span>
<span class="go">       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span>
<span class="go">       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span>
<span class="go">       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span>
<span class="go">       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coords</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coords_subpix</span> <span class="o">=</span> <span class="n">corner_subpix</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">coords</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coords_subpix</span>
<span class="go">array([[4.5, 4.5]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this simplified example we first generate two synthetic images as if they were taken from different view points."><img alt="" src="../_images/sphx_glr_plot_matching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_matching.html#sphx-glr-auto-examples-transform-plot-matching-py"><span class="std std-ref">Robust matching using RANSAC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust matching using RANSAC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Detect corner points using the Harris corner detector and determine the subpixel position of corners ([1]_, [2]_)."><img alt="" src="../_images/sphx_glr_plot_corner_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_corner.html#sphx-glr-auto-examples-features-detection-plot-corner-py"><span class="std std-ref">Corner detection</span></a></p>
  <div class="sphx-glr-thumbnail-title">Corner detection</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.daisy">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">daisy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">histograms</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">orientations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigmas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ring_radii</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">visualize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_daisy.py#L13-L249"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.daisy" title="Link to this definition">#</a></dt>
<dd><p>Extract DAISY feature descriptors densely for the given image.</p>
<p>DAISY is a feature descriptor similar to SIFT formulated in a way that
allows for fast dense extraction. Typically, this is practical for
bag-of-features image representations.</p>
<p>The implementation follows Tola et al. <a class="reference internal" href="#r3f18658b3c6d-1" id="id25">[1]</a> but deviate on the following
points:</p>
<blockquote>
<div><ul class="simple">
<li><p>Histogram bin contribution are smoothed with a circular Gaussian
window over the tonal range (the angular range).</p></li>
<li><p>The sigma values of the spatial Gaussian smoothing in this code do not
match the sigma values in the original code by Tola et al. <a class="reference internal" href="#r3f18658b3c6d-2" id="id26">[2]</a>. In
their code, spatial smoothing is applied to both the input image and
the center histogram. However, this smoothing is not documented in <a class="reference internal" href="#r3f18658b3c6d-1" id="id27">[1]</a>
and, therefore, it is omitted.</p></li>
</ul>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">(M, N) array</span></dt><dd><p>Input image (grayscale).</p>
</dd>
<dt><strong>step</strong><span class="classifier">int, optional</span></dt><dd><p>Distance between descriptor sampling points.</p>
</dd>
<dt><strong>radius</strong><span class="classifier">int, optional</span></dt><dd><p>Radius (in pixels) of the outermost ring.</p>
</dd>
<dt><strong>rings</strong><span class="classifier">int, optional</span></dt><dd><p>Number of rings.</p>
</dd>
<dt><strong>histograms</strong><span class="classifier">int, optional</span></dt><dd><p>Number of histograms sampled per ring.</p>
</dd>
<dt><strong>orientations</strong><span class="classifier">int, optional</span></dt><dd><p>Number of orientations (bins) per histogram.</p>
</dd>
<dt><strong>normalization</strong><span class="classifier">[ ‘l1’ | ‘l2’ | ‘daisy’ | ‘off’ ], optional</span></dt><dd><p>How to normalize the descriptors</p>
<blockquote>
<div><ul class="simple">
<li><p>‘l1’: L1-normalization of each descriptor.</p></li>
<li><p>‘l2’: L2-normalization of each descriptor.</p></li>
<li><p>‘daisy’: L2-normalization of individual histograms.</p></li>
<li><p>‘off’: Disable normalization.</p></li>
</ul>
</div></blockquote>
</dd>
<dt><strong>sigmas</strong><span class="classifier">1D array of float, optional</span></dt><dd><p>Standard deviation of spatial Gaussian smoothing for the center
histogram and for each ring of histograms. The array of sigmas should
be sorted from the center and out. I.e. the first sigma value defines
the spatial smoothing of the center histogram and the last sigma value
defines the spatial smoothing of the outermost ring. Specifying sigmas
overrides the following parameter.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">rings</span> <span class="pre">=</span> <span class="pre">len(sigmas)</span> <span class="pre">-</span> <span class="pre">1</span></code></p>
</div></blockquote>
</dd>
<dt><strong>ring_radii</strong><span class="classifier">1D array of int, optional</span></dt><dd><p>Radius (in pixels) for each ring. Specifying ring_radii overrides the
following two parameters.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">rings</span> <span class="pre">=</span> <span class="pre">len(ring_radii)</span></code>
<code class="docutils literal notranslate"><span class="pre">radius</span> <span class="pre">=</span> <span class="pre">ring_radii[-1]</span></code></p>
</div></blockquote>
<p>If both sigmas and ring_radii are given, they must satisfy the
following predicate since no radius is needed for the center
histogram.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">len(ring_radii)</span> <span class="pre">==</span> <span class="pre">len(sigmas)</span> <span class="pre">+</span> <span class="pre">1</span></code></p>
</div></blockquote>
</dd>
<dt><strong>visualize</strong><span class="classifier">bool, optional</span></dt><dd><p>Generate a visualization of the DAISY descriptors</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>descs</strong><span class="classifier">array</span></dt><dd><p>Grid of DAISY descriptors for the given image as an array
dimensionality  (P, Q, R) where</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">P</span> <span class="pre">=</span> <span class="pre">ceil((M</span> <span class="pre">-</span> <span class="pre">radius*2)</span> <span class="pre">/</span> <span class="pre">step)</span></code>
<code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">=</span> <span class="pre">ceil((N</span> <span class="pre">-</span> <span class="pre">radius*2)</span> <span class="pre">/</span> <span class="pre">step)</span></code>
<code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">=</span> <span class="pre">(rings</span> <span class="pre">*</span> <span class="pre">histograms</span> <span class="pre">+</span> <span class="pre">1)</span> <span class="pre">*</span> <span class="pre">orientations</span></code></p>
</div></blockquote>
</dd>
<dt><strong>descs_img</strong><span class="classifier">(M, N, 3) array (only if visualize==True)</span></dt><dd><p>Visualization of the DAISY descriptors.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r3f18658b3c6d-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id25">1</a>,<a role="doc-backlink" href="#id27">2</a>)</span>
<p>Tola et al. “Daisy: An efficient dense descriptor applied to wide-
baseline stereo.” Pattern Analysis and Machine Intelligence, IEEE
Transactions on 32.5 (2010): 815-830.</p>
</div>
<div class="citation" id="r3f18658b3c6d-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://cvlab.epfl.ch/software/daisy">http://cvlab.epfl.ch/software/daisy</a></p>
</div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The DAISY local image descriptor is based on gradient orientation histograms similar to the SIFT descriptor. It is formulated in a way that allows for fast dense extraction which is useful for e.g. bag-of-features image representations."><img alt="" src="../_images/sphx_glr_plot_daisy_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_daisy.html#sphx-glr-auto-examples-features-detection-plot-daisy-py"><span class="std std-ref">Dense DAISY feature description</span></a></p>
  <div class="sphx-glr-thumbnail-title">Dense DAISY feature description</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.draw_haar_like_feature">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">draw_haar_like_feature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_coord</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_positive_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1.0,</span> <span class="pre">0.0,</span> <span class="pre">0.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_negative_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.0,</span> <span class="pre">1.0,</span> <span class="pre">0.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/haar.py#L235-L339"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.draw_haar_like_feature" title="Link to this definition">#</a></dt>
<dd><p>Visualization of Haar-like features.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>The region of an integral image for which the features need to be
computed.</p>
</dd>
<dt><strong>r</strong><span class="classifier">int</span></dt><dd><p>Row-coordinate of top left corner of the detection window.</p>
</dd>
<dt><strong>c</strong><span class="classifier">int</span></dt><dd><p>Column-coordinate of top left corner of the detection window.</p>
</dd>
<dt><strong>width</strong><span class="classifier">int</span></dt><dd><p>Width of the detection window.</p>
</dd>
<dt><strong>height</strong><span class="classifier">int</span></dt><dd><p>Height of the detection window.</p>
</dd>
<dt><strong>feature_coord</strong><span class="classifier">ndarray of list of tuples or None, optional</span></dt><dd><p>The array of coordinates to be extracted. This is useful when you want
to recompute only a subset of features. In this case <code class="xref py py-obj docutils literal notranslate"><span class="pre">feature_type</span></code>
needs to be an array containing the type of each feature, as returned
by <a class="reference internal" href="#skimage.feature.haar_like_feature_coord" title="skimage.feature.haar_like_feature_coord"><code class="xref py py-func docutils literal notranslate"><span class="pre">haar_like_feature_coord()</span></code></a>. By default, all coordinates are
computed.</p>
</dd>
<dt><strong>color_positive_block</strong><span class="classifier">tuple of 3 floats</span></dt><dd><p>Floats specifying the color for the positive block. Corresponding
values define (R, G, B) values. Default value is red (1, 0, 0).</p>
</dd>
<dt><strong>color_negative_block</strong><span class="classifier">tuple of 3 floats</span></dt><dd><p>Floats specifying the color for the negative block Corresponding values
define (R, G, B) values. Default value is blue (0, 1, 0).</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float</span></dt><dd><p>Value in the range [0, 1] that specifies opacity of visualization. 1 -
fully transparent, 0 - opaque.</p>
</dd>
<dt><strong>max_n_features</strong><span class="classifier">int, default=None</span></dt><dd><p>The maximum number of features to be returned.
By default, all features are returned.</p>
</dd>
<dt><strong>rng</strong><span class="classifier">{<a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator" title="(in NumPy v2.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.random.Generator</span></code></a>, int}, optional</span></dt><dd><p>Pseudo-random number generator.
By default, a PCG64 generator is used (see <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng" title="(in NumPy v2.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.random.default_rng()</span></code></a>).
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">rng</span></code> is an int, it is used to seed the generator.</p>
<p>The rng is used when generating a set of features smaller than
the total number of available features.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>features</strong><span class="classifier">(M, N), ndarray</span></dt><dd><p>An image in which the different features will be added.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">haar_like_feature_coord</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">draw_haar_like_feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_coord</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">haar_like_feature_coord</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;type-4&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">draw_haar_like_feature</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
<span class="gp">... </span>                               <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                               <span class="n">feature_coord</span><span class="p">,</span>
<span class="gp">... </span>                               <span class="n">max_n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span>
<span class="go">array([[[0. , 0.5, 0. ],</span>
<span class="go">        [0.5, 0. , 0. ]],</span>

<span class="go">       [[0.5, 0. , 0. ],</span>
<span class="go">        [0. , 0.5, 0. ]]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Haar-like features are simple digital image features that were introduced in a real-time face detector [1]_. These features can be efficiently computed on any scale in constant time, using an integral image [1]_. After that, a small number of critical features is selected from this large set of potential features (e.g., using AdaBoost learning algorithm as in [1]_). The following example will show the mechanism to build this family of descriptors."><img alt="" src="../_images/sphx_glr_plot_haar_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_haar.html#sphx-glr-auto-examples-features-detection-plot-haar-py"><span class="std std-ref">Haar-like feature descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">Haar-like feature descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Haar-like feature descriptors were successfully used to implement the first real-time face detector [1]_. Inspired by this application, we propose an example illustrating the extraction, selection, and classification of Haar-like features to detect faces vs. non-faces."><img alt="" src="../_images/sphx_glr_plot_haar_extraction_selection_classification_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py"><span class="std std-ref">Face classification using Haar-like feature descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">Face classification using Haar-like feature descriptor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.draw_multiblock_lbp">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">draw_multiblock_lbp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lbp_code</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_greater_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_less_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0,</span> <span class="pre">0.69,</span> <span class="pre">0.96)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/texture.py#L443-L562"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.draw_multiblock_lbp" title="Link to this definition">#</a></dt>
<dd><p>Multi-block local binary pattern visualization.</p>
<p>Blocks with higher sums are colored with alpha-blended white rectangles,
whereas blocks with lower sums are colored alpha-blended cyan. Colors
and the <code class="xref py py-obj docutils literal notranslate"><span class="pre">alpha</span></code> parameter can be changed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray of float or uint</span></dt><dd><p>Image on which to visualize the pattern.</p>
</dd>
<dt><strong>r</strong><span class="classifier">int</span></dt><dd><p>Row-coordinate of top left corner of a rectangle containing feature.</p>
</dd>
<dt><strong>c</strong><span class="classifier">int</span></dt><dd><p>Column-coordinate of top left corner of a rectangle containing feature.</p>
</dd>
<dt><strong>width</strong><span class="classifier">int</span></dt><dd><p>Width of one of 9 equal rectangles that will be used to compute
a feature.</p>
</dd>
<dt><strong>height</strong><span class="classifier">int</span></dt><dd><p>Height of one of 9 equal rectangles that will be used to compute
a feature.</p>
</dd>
<dt><strong>lbp_code</strong><span class="classifier">int</span></dt><dd><p>The descriptor of feature to visualize. If not provided, the
descriptor with 0 value will be used.</p>
</dd>
<dt><strong>color_greater_block</strong><span class="classifier">tuple of 3 floats</span></dt><dd><p>Floats specifying the color for the block that has greater
intensity value. They should be in the range [0, 1].
Corresponding values define (R, G, B) values. Default value
is white (1, 1, 1).</p>
</dd>
<dt><strong>color_greater_block</strong><span class="classifier">tuple of 3 floats</span></dt><dd><p>Floats specifying the color for the block that has greater intensity
value. They should be in the range [0, 1]. Corresponding values define
(R, G, B) values. Default value is cyan (0, 0.69, 0.96).</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float</span></dt><dd><p>Value in the range [0, 1] that specifies opacity of visualization.
1 - fully transparent, 0 - opaque.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">ndarray of float</span></dt><dd><p>Image with MB-LBP visualization.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="re2978af1b6c8-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>L. Zhang, R. Chu, S. Xiang, S. Liao, S.Z. Li. “Face Detection Based
on Multi-Block LBP Representation”, In Proceedings: Advances in
Biometrics, International Conference, ICB 2007, Seoul, Korea.
<a class="reference external" href="http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf">http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf</a>
<a class="reference external" href="https://doi.org/10.1007/978-3-540-74549-5_2">DOI:10.1007/978-3-540-74549-5_2</a></p>
</div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to compute multi-block local binary pattern (MB-LBP) features as well as how to visualize them."><img alt="" src="../_images/sphx_glr_plot_multiblock_local_binary_pattern_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_multiblock_local_binary_pattern.html#sphx-glr-auto-examples-features-detection-plot-multiblock-local-binary-pattern-py"><span class="std std-ref">Multi-Block Local Binary Pattern for texture classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multi-Block Local Binary Pattern for texture classification</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.fisher_vector">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">fisher_vector</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">descriptors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gmm</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">improved</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_fisher_vector.py#L155-L265"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.fisher_vector" title="Link to this definition">#</a></dt>
<dd><p>Compute the Fisher vector given some descriptors/vectors,
and an associated estimated GMM.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>descriptors</strong><span class="classifier">np.ndarray, shape=(n_descriptors, descriptor_length)</span></dt><dd><p>NumPy array of the descriptors for which the Fisher vector
representation is to be computed.</p>
</dd>
<dt><strong>gmm</strong><span class="classifier"><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="(in scikit-learn v1.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.mixture.GaussianMixture</span></code></a></span></dt><dd><p>An estimated GMM object, which contains the necessary parameters needed
to compute the Fisher vector.</p>
</dd>
<dt><strong>improved</strong><span class="classifier">bool, default=False</span></dt><dd><p>Flag denoting whether to compute improved Fisher vectors or not.
Improved Fisher vectors are L2 and power normalized. Power
normalization is simply f(z) = sign(z) pow(abs(z), alpha) for some
0 &lt;= alpha &lt;= 1.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, default=0.5</span></dt><dd><p>The parameter for the power normalization step. Ignored if
improved=False.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>fisher_vector</strong><span class="classifier">np.ndarray</span></dt><dd><p>The computation Fisher vector, which is given by a concatenation of the
gradients of a GMM with respect to its parameters (mixture weights,
means, and covariance matrices). For D-dimensional input descriptors or
vectors, and a K-mode GMM, the Fisher vector dimensionality will be
2KD + K. Thus, its dimensionality is invariant to the number of
descriptors/vectors.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r53332c0dacd7-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Perronnin, F. and Dance, C. Fisher kernels on Visual Vocabularies
for Image Categorization, IEEE Conference on Computer Vision and
Pattern Recognition, 2007</p>
</div>
<div class="citation" id="r53332c0dacd7-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Perronnin, F. and Sanchez, J. and Mensink T. Improving the Fisher
Kernel for Large-Scale Image Classification, ECCV, 2010</p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">fisher_vector</span><span class="p">,</span> <span class="n">learn_gmm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sift_for_images</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_modes</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Estimate 16-mode GMM with these synthetic SIFT vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gmm</span> <span class="o">=</span> <span class="n">learn_gmm</span><span class="p">(</span><span class="n">sift_for_images</span><span class="p">,</span> <span class="n">n_modes</span><span class="o">=</span><span class="n">num_modes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_image_descriptors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">25</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute the Fisher vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fv</span> <span class="o">=</span> <span class="n">fisher_vector</span><span class="p">(</span><span class="n">test_image_descriptors</span><span class="p">,</span> <span class="n">gmm</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A Fisher vector is an image feature encoding and quantization technique that can be seen as a soft or probabilistic version of the popular bag-of-visual-words or VLAD algorithms. Images are modelled using a visual vocabulary which is estimated using a K-mode Gaussian mixture model trained on low-level image features such as SIFT or ORB descriptors. The Fisher vector itself is a concatenation of the gradients of the Gaussian mixture model (GMM) with respect to its parameters - mixture weights, means, and covariance matrices."><img alt="" src="../_images/sphx_glr_plot_fisher_vector_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_fisher_vector.html#sphx-glr-auto-examples-features-detection-plot-fisher-vector-py"><span class="std std-ref">Fisher vector feature encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fisher vector feature encoding</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.graycomatrix">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">graycomatrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distances</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">angles</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">levels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/texture.py#L15-L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.graycomatrix" title="Link to this definition">#</a></dt>
<dd><p>Calculate the gray-level co-occurrence matrix.</p>
<p>A gray level co-occurrence matrix is a histogram of co-occurring
grayscale values at a given offset over an image.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span><code class="xref py py-obj docutils literal notranslate"><span class="pre">greymatrix</span></code> was renamed to <code class="xref py py-obj docutils literal notranslate"><span class="pre">graymatrix</span></code> in 0.19.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">array_like</span></dt><dd><p>Integer typed input image. Only positive valued images are supported.
If type is other than uint8, the argument <code class="xref py py-obj docutils literal notranslate"><span class="pre">levels</span></code> needs to be set.</p>
</dd>
<dt><strong>distances</strong><span class="classifier">array_like</span></dt><dd><p>List of pixel pair distance offsets.</p>
</dd>
<dt><strong>angles</strong><span class="classifier">array_like</span></dt><dd><p>List of pixel pair angles in radians.</p>
</dd>
<dt><strong>levels</strong><span class="classifier">int, optional</span></dt><dd><p>The input image should contain integers in [0, <code class="xref py py-obj docutils literal notranslate"><span class="pre">levels</span></code>-1],
where levels indicate the number of gray-levels counted
(typically 256 for an 8-bit image). This argument is required for
16-bit images or higher and is typically the maximum of the image.
As the output matrix is at least <code class="xref py py-obj docutils literal notranslate"><span class="pre">levels</span></code> x <code class="xref py py-obj docutils literal notranslate"><span class="pre">levels</span></code>, it might
be preferable to use binning of the input image rather than
large values for <code class="xref py py-obj docutils literal notranslate"><span class="pre">levels</span></code>.</p>
</dd>
<dt><strong>symmetric</strong><span class="classifier">bool, optional</span></dt><dd><p>If True, the output matrix <code class="xref py py-obj docutils literal notranslate"><span class="pre">P[:,</span> <span class="pre">:,</span> <span class="pre">d,</span> <span class="pre">theta]</span></code> is symmetric. This
is accomplished by ignoring the order of value pairs, so both
(i, j) and (j, i) are accumulated when (i, j) is encountered
for a given offset. The default is False.</p>
</dd>
<dt><strong>normed</strong><span class="classifier">bool, optional</span></dt><dd><p>If True, normalize each matrix <code class="xref py py-obj docutils literal notranslate"><span class="pre">P[:,</span> <span class="pre">:,</span> <span class="pre">d,</span> <span class="pre">theta]</span></code> by dividing
by the total number of accumulated co-occurrences for the given
offset. The elements of the resulting matrix sum to 1. The
default is False.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>P</strong><span class="classifier">4-D ndarray</span></dt><dd><p>The gray-level co-occurrence histogram. The value
<code class="xref py py-obj docutils literal notranslate"><span class="pre">P[i,j,d,theta]</span></code> is the number of times that gray-level <code class="xref py py-obj docutils literal notranslate"><span class="pre">j</span></code>
occurs at a distance <code class="xref py py-obj docutils literal notranslate"><span class="pre">d</span></code> and at an angle <code class="xref py py-obj docutils literal notranslate"><span class="pre">theta</span></code> from
gray-level <code class="xref py py-obj docutils literal notranslate"><span class="pre">i</span></code>. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">normed</span></code> is <a class="reference external" href="https://docs.python.org/3/library/constants.html#False" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">False</span></code></a>, the output is of
type uint32, otherwise it is float64. The dimensions are:
levels x levels x number of distances x number of angles.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r175a60c63ca5-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>M. Hall-Beyer, 2007. GLCM Texture: A Tutorial
<a class="reference external" href="https://prism.ucalgary.ca/handle/1880/51900">https://prism.ucalgary.ca/handle/1880/51900</a>
DOI:<code class="xref py py-obj docutils literal notranslate"><span class="pre">10.11575/PRISM/33280</span></code></p>
</div>
<div class="citation" id="r175a60c63ca5-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>R.M. Haralick, K. Shanmugam, and I. Dinstein, “Textural features for
image classification”, IEEE Transactions on Systems, Man, and
Cybernetics, vol. SMC-3, no. 6, pp. 610-621, Nov. 1973.
<a class="reference external" href="https://doi.org/10.1109/TSMC.1973.4309314">DOI:10.1109/TSMC.1973.4309314</a></p>
</div>
<div class="citation" id="r175a60c63ca5-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>M. Nadler and E.P. Smith, Pattern Recognition Engineering,
Wiley-Interscience, 1993.</p>
</div>
<div class="citation" id="r175a60c63ca5-4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<p>Wikipedia, <a class="reference external" href="https://en.wikipedia.org/wiki/Co-occurrence_matrix">https://en.wikipedia.org/wiki/Co-occurrence_matrix</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<p>Compute 4 GLCMs using 1-pixel distance and 4 different angles. For example,
an angle of 0 radians refers to the neighboring pixel to the right;
pi/4 radians to the top-right diagonal neighbor; pi/2 radians to the pixel
above, and so forth.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">graycomatrix</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">],</span>
<span class="gp">... </span>                      <span class="n">levels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">array([[2, 2, 1, 0],</span>
<span class="go">       [0, 2, 0, 0],</span>
<span class="go">       [0, 0, 3, 1],</span>
<span class="go">       [0, 0, 0, 1]], dtype=uint32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="go">array([[1, 1, 3, 0],</span>
<span class="go">       [0, 1, 1, 0],</span>
<span class="go">       [0, 0, 0, 2],</span>
<span class="go">       [0, 0, 0, 0]], dtype=uint32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="go">array([[3, 0, 2, 0],</span>
<span class="go">       [0, 2, 2, 0],</span>
<span class="go">       [0, 0, 1, 2],</span>
<span class="go">       [0, 0, 0, 0]], dtype=uint32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="go">array([[2, 0, 0, 0],</span>
<span class="go">       [1, 1, 2, 0],</span>
<span class="go">       [0, 0, 2, 1],</span>
<span class="go">       [0, 0, 0, 0]], dtype=uint32)</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates texture classification using gray level co-occurrence matrices (GLCMs) [1]_. A GLCM is a histogram of co-occurring grayscale values at a given offset over an image."><img alt="" src="../_images/sphx_glr_plot_glcm_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_glcm.html#sphx-glr-auto-examples-features-detection-plot-glcm-py"><span class="std std-ref">GLCM Texture Features</span></a></p>
  <div class="sphx-glr-thumbnail-title">GLCM Texture Features</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.graycoprops">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">graycoprops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'contrast'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/texture.py#L170-L315"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.graycoprops" title="Link to this definition">#</a></dt>
<dd><p>Calculate texture properties of a GLCM.</p>
<p>Compute a feature of a gray level co-occurrence matrix to serve as
a compact summary of the matrix. The properties are computed as
follows:</p>
<ul>
<li><p>‘contrast’: <span class="math notranslate nohighlight">\(\sum_{i,j=0}^{levels-1} P_{i,j}(i-j)^2\)</span></p></li>
<li><p>‘dissimilarity’: <span class="math notranslate nohighlight">\(\sum_{i,j=0}^{levels-1}P_{i,j}|i-j|\)</span></p></li>
<li><p>‘homogeneity’: <span class="math notranslate nohighlight">\(\sum_{i,j=0}^{levels-1}\frac{P_{i,j}}{1+(i-j)^2}\)</span></p></li>
<li><p>‘ASM’: <span class="math notranslate nohighlight">\(\sum_{i,j=0}^{levels-1} P_{i,j}^2\)</span></p></li>
<li><p>‘energy’: <span class="math notranslate nohighlight">\(\sqrt{ASM}\)</span></p></li>
<li><dl>
<dt>‘correlation’:</dt><dd><div class="math notranslate nohighlight">
\[\sum_{i,j=0}^{levels-1} P_{i,j}\left[\frac{(i-\mu_i) \
(j-\mu_j)}{\sqrt{(\sigma_i^2)(\sigma_j^2)}}\right]\]</div>
</dd>
</dl>
</li>
<li><p>‘mean’: <span class="math notranslate nohighlight">\(\sum_{i=0}^{levels-1} i*P_{i}\)</span></p></li>
<li><p>‘variance’: <span class="math notranslate nohighlight">\(\sum_{i=0}^{levels-1} P_{i}*(i-mean)^2\)</span></p></li>
<li><p>‘std’: <span class="math notranslate nohighlight">\(\sqrt{variance}\)</span></p></li>
<li><p>‘entropy’: <span class="math notranslate nohighlight">\(\sum_{i,j=0}^{levels-1} -P_{i,j}*log(P_{i,j})\)</span></p></li>
</ul>
<p>Each GLCM is normalized to have a sum of 1 before the computation of
texture properties.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span><code class="xref py py-obj docutils literal notranslate"><span class="pre">greycoprops</span></code> was renamed to <a class="reference internal" href="#skimage.feature.graycoprops" title="skimage.feature.graycoprops"><code class="xref py py-obj docutils literal notranslate"><span class="pre">graycoprops</span></code></a> in 0.19.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>P</strong><span class="classifier">ndarray</span></dt><dd><p>Input array. <code class="xref py py-obj docutils literal notranslate"><span class="pre">P</span></code> is the gray-level co-occurrence histogram
for which to compute the specified property. The value
<code class="xref py py-obj docutils literal notranslate"><span class="pre">P[i,j,d,theta]</span></code> is the number of times that gray-level j
occurs at a distance d and at an angle theta from
gray-level i.</p>
</dd>
<dt><strong>prop</strong><span class="classifier">{‘contrast’, ‘dissimilarity’, ‘homogeneity’, ‘energy’,             ‘correlation’, ‘ASM’, ‘mean’, ‘variance’, ‘std’, ‘entropy’}, optional</span></dt><dd><p>The property of the GLCM to compute. The default is ‘contrast’.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>results</strong><span class="classifier">2-D ndarray</span></dt><dd><p>2-dimensional array. <code class="xref py py-obj docutils literal notranslate"><span class="pre">results[d,</span> <span class="pre">a]</span></code> is the property ‘prop’ for
the d’th distance and the a’th angle.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rf54815ec3382-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>M. Hall-Beyer, 2007. GLCM Texture: A Tutorial v. 1.0 through 3.0.
The GLCM Tutorial Home Page,
<a class="reference external" href="https://prism.ucalgary.ca/handle/1880/51900">https://prism.ucalgary.ca/handle/1880/51900</a>
DOI:<code class="xref py py-obj docutils literal notranslate"><span class="pre">10.11575/PRISM/33280</span></code></p>
</div>
</div>
<p class="rubric">Examples</p>
<p>Compute the contrast for GLCMs with distances [1, 2] and angles
[0 degrees, 90 degrees]</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="gp">... </span>                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">g</span> <span class="o">=</span> <span class="n">graycomatrix</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span> <span class="n">levels</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">normed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contrast</span> <span class="o">=</span> <span class="n">graycoprops</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s1">&#39;contrast&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contrast</span>
<span class="go">array([[0.58333333, 1.        ],</span>
<span class="go">       [1.25      , 2.75      ]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example illustrates texture classification using gray level co-occurrence matrices (GLCMs) [1]_. A GLCM is a histogram of co-occurring grayscale values at a given offset over an image."><img alt="" src="../_images/sphx_glr_plot_glcm_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_glcm.html#sphx-glr-auto-examples-features-detection-plot-glcm-py"><span class="std std-ref">GLCM Texture Features</span></a></p>
  <div class="sphx-glr-thumbnail-title">GLCM Texture Features</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.haar_like_feature">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">haar_like_feature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">int_image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_coord</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/haar.py#L86-L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.haar_like_feature" title="Link to this definition">#</a></dt>
<dd><p>Compute the Haar-like features for a region of interest (ROI) of an
integral image.</p>
<p>Haar-like features have been successfully used for image classification and
object detection <a class="reference internal" href="#rbcb83f52fce4-1" id="id38">[1]</a>. It has been used for real-time face detection
algorithm proposed in <a class="reference internal" href="#rbcb83f52fce4-2" id="id39">[2]</a>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>int_image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Integral image for which the features need to be computed.</p>
</dd>
<dt><strong>r</strong><span class="classifier">int</span></dt><dd><p>Row-coordinate of top left corner of the detection window.</p>
</dd>
<dt><strong>c</strong><span class="classifier">int</span></dt><dd><p>Column-coordinate of top left corner of the detection window.</p>
</dd>
<dt><strong>width</strong><span class="classifier">int</span></dt><dd><p>Width of the detection window.</p>
</dd>
<dt><strong>height</strong><span class="classifier">int</span></dt><dd><p>Height of the detection window.</p>
</dd>
<dt><strong>feature_type</strong><span class="classifier">str or list of str or None, optional</span></dt><dd><p>The type of feature to consider:</p>
<ul class="simple">
<li><p>‘type-2-x’: 2 rectangles varying along the x axis;</p></li>
<li><p>‘type-2-y’: 2 rectangles varying along the y axis;</p></li>
<li><p>‘type-3-x’: 3 rectangles varying along the x axis;</p></li>
<li><p>‘type-3-y’: 3 rectangles varying along the y axis;</p></li>
<li><p>‘type-4’: 4 rectangles varying along x and y axis.</p></li>
</ul>
<p>By default all features are extracted.</p>
<p>If using with <code class="xref py py-obj docutils literal notranslate"><span class="pre">feature_coord</span></code>, it should correspond to the feature
type of each associated coordinate feature.</p>
</dd>
<dt><strong>feature_coord</strong><span class="classifier">ndarray of list of tuples or None, optional</span></dt><dd><p>The array of coordinates to be extracted. This is useful when you want
to recompute only a subset of features. In this case <code class="xref py py-obj docutils literal notranslate"><span class="pre">feature_type</span></code>
needs to be an array containing the type of each feature, as returned
by <a class="reference internal" href="#skimage.feature.haar_like_feature_coord" title="skimage.feature.haar_like_feature_coord"><code class="xref py py-func docutils literal notranslate"><span class="pre">haar_like_feature_coord()</span></code></a>. By default, all coordinates are
computed.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>haar_features</strong><span class="classifier">(n_features,) ndarray of int or float</span></dt><dd><p>Resulting Haar-like features. Each value is equal to the subtraction of
sums of the positive and negative rectangles. The data type depends of
the data type of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int_image</span></code>: <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> when the data type of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int_image</span></code>
is <code class="xref py py-obj docutils literal notranslate"><span class="pre">uint</span></code> or <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code></a> and <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a> when the data type of <code class="xref py py-obj docutils literal notranslate"><span class="pre">int_image</span></code> is
<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>When extracting those features in parallel, be aware that the choice of the
backend (i.e. multiprocessing vs threading) will have an impact on the
performance. The rule of thumb is as follows: use multiprocessing when
extracting features for all possible ROI in an image; use threading when
extracting the feature at specific location for a limited number of ROIs.
Refer to the example
<a class="reference internal" href="../auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py"><span class="std std-ref">Face classification using Haar-like feature descriptor</span></a>
for more insights.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rbcb83f52fce4-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id38">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Haar-like_feature">https://en.wikipedia.org/wiki/Haar-like_feature</a></p>
</div>
<div class="citation" id="rbcb83f52fce4-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id39">2</a><span class="fn-bracket">]</span></span>
<p>Oren, M., Papageorgiou, C., Sinha, P., Osuna, E., &amp; Poggio, T.
(1997, June). Pedestrian detection using wavelet templates.
In Computer Vision and Pattern Recognition, 1997. Proceedings.,
1997 IEEE Computer Society Conference on (pp. 193-199). IEEE.
<a class="reference external" href="http://tinyurl.com/y6ulxfta">http://tinyurl.com/y6ulxfta</a>
<a class="reference external" href="https://doi.org/10.1109/CVPR.1997.609319">DOI:10.1109/CVPR.1997.609319</a></p>
</div>
<div class="citation" id="rbcb83f52fce4-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Viola, Paul, and Michael J. Jones. “Robust real-time face
detection.” International journal of computer vision 57.2
(2004): 137-154.
<a class="reference external" href="https://www.merl.com/publications/docs/TR2004-043.pdf">https://www.merl.com/publications/docs/TR2004-043.pdf</a>
<a class="reference external" href="https://doi.org/10.1109/CVPR.2001.990517">DOI:10.1109/CVPR.2001.990517</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.transform</span> <span class="kn">import</span> <span class="n">integral_image</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">haar_like_feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img_ii</span> <span class="o">=</span> <span class="n">integral_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span> <span class="o">=</span> <span class="n">haar_like_feature</span><span class="p">(</span><span class="n">img_ii</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;type-3-x&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span>
<span class="go">array([-1, -2, -3, -4, -5, -1, -2, -3, -4, -5, -1, -2, -3, -4, -5, -1, -2,</span>
<span class="go">       -3, -4, -1, -2, -3, -4, -1, -2, -3, -4, -1, -2, -3, -1, -2, -3, -1,</span>
<span class="go">       -2, -3, -1, -2, -1, -2, -1, -2, -1, -1, -1])</span>
</pre></div>
</div>
<p>You can compute the feature for some pre-computed coordinates.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">haar_like_feature_coord</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_coord</span><span class="p">,</span> <span class="n">feature_type</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
<span class="gp">... </span>    <span class="o">*</span><span class="p">[</span><span class="n">haar_like_feature_coord</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">feat_t</span><span class="p">)</span>
<span class="gp">... </span>      <span class="k">for</span> <span class="n">feat_t</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;type-2-x&#39;</span><span class="p">,</span> <span class="s1">&#39;type-3-x&#39;</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># only select one feature over two</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_coord</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">feature_coord</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_type</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">[::</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">feature_type</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span> <span class="o">=</span> <span class="n">haar_like_feature</span><span class="p">(</span><span class="n">img_ii</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">feature_type</span><span class="o">=</span><span class="n">feature_type</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">feature_coord</span><span class="o">=</span><span class="n">feature_coord</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span>
<span class="go">array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,</span>
<span class="go">        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,</span>
<span class="go">        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -3, -5, -2, -4, -1,</span>
<span class="go">       -3, -5, -2, -4, -2, -4, -2, -4, -2, -1, -3, -2, -1, -1, -1, -1, -1])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Haar-like feature descriptors were successfully used to implement the first real-time face detector [1]_. Inspired by this application, we propose an example illustrating the extraction, selection, and classification of Haar-like features to detect faces vs. non-faces."><img alt="" src="../_images/sphx_glr_plot_haar_extraction_selection_classification_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py"><span class="std std-ref">Face classification using Haar-like feature descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">Face classification using Haar-like feature descriptor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.haar_like_feature_coord">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">haar_like_feature_coord</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/haar.py#L33-L83"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.haar_like_feature_coord" title="Link to this definition">#</a></dt>
<dd><p>Compute the coordinates of Haar-like features.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>width</strong><span class="classifier">int</span></dt><dd><p>Width of the detection window.</p>
</dd>
<dt><strong>height</strong><span class="classifier">int</span></dt><dd><p>Height of the detection window.</p>
</dd>
<dt><strong>feature_type</strong><span class="classifier">str or list of str or None, optional</span></dt><dd><p>The type of feature to consider:</p>
<ul class="simple">
<li><p>‘type-2-x’: 2 rectangles varying along the x axis;</p></li>
<li><p>‘type-2-y’: 2 rectangles varying along the y axis;</p></li>
<li><p>‘type-3-x’: 3 rectangles varying along the x axis;</p></li>
<li><p>‘type-3-y’: 3 rectangles varying along the y axis;</p></li>
<li><p>‘type-4’: 4 rectangles varying along x and y axis.</p></li>
</ul>
<p>By default all features are extracted.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>feature_coord</strong><span class="classifier">(n_features, n_rectangles, 2, 2), ndarray of list of tuple coord</span></dt><dd><p>Coordinates of the rectangles for each feature.</p>
</dd>
<dt><strong>feature_type</strong><span class="classifier">(n_features,), ndarray of str</span></dt><dd><p>The corresponding type for each feature.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.transform</span> <span class="kn">import</span> <span class="n">integral_image</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">haar_like_feature_coord</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feat_coord</span><span class="p">,</span> <span class="n">feat_type</span> <span class="o">=</span> <span class="n">haar_like_feature_coord</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;type-4&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feat_coord</span> 
<span class="go">array([ list([[(0, 0), (0, 0)], [(0, 1), (0, 1)],</span>
<span class="go">              [(1, 1), (1, 1)], [(1, 0), (1, 0)]])], dtype=object)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feat_type</span>
<span class="go">array([&#39;type-4&#39;], dtype=object)</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Haar-like features are simple digital image features that were introduced in a real-time face detector [1]_. These features can be efficiently computed on any scale in constant time, using an integral image [1]_. After that, a small number of critical features is selected from this large set of potential features (e.g., using AdaBoost learning algorithm as in [1]_). The following example will show the mechanism to build this family of descriptors."><img alt="" src="../_images/sphx_glr_plot_haar_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_haar.html#sphx-glr-auto-examples-features-detection-plot-haar-py"><span class="std std-ref">Haar-like feature descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">Haar-like feature descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Haar-like feature descriptors were successfully used to implement the first real-time face detector [1]_. Inspired by this application, we propose an example illustrating the extraction, selection, and classification of Haar-like features to detect faces vs. non-faces."><img alt="" src="../_images/sphx_glr_plot_haar_extraction_selection_classification_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_haar_extraction_selection_classification.html#sphx-glr-auto-examples-applications-plot-haar-extraction-selection-classification-py"><span class="std std-ref">Face classification using Haar-like feature descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">Face classification using Haar-like feature descriptor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.hessian_matrix">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">hessian_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rc'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_gaussian_derivatives</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L225-L338"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.hessian_matrix" title="Link to this definition">#</a></dt>
<dd><p>Compute the Hessian matrix.</p>
<p>In 2D, the Hessian matrix is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="p">[</span><span class="n">Hrr</span> <span class="n">Hrc</span><span class="p">]</span>
    <span class="p">[</span><span class="n">Hrc</span> <span class="n">Hcc</span><span class="p">]</span>
</pre></div>
</div>
<p>which is computed by convolving the image with the second derivatives
of the Gaussian kernel in the respective r- and c-directions.</p>
<p>The implementation here also supports n-dimensional data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used as
weighting function for the auto-correlation matrix.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">{‘constant’, ‘reflect’, ‘wrap’, ‘nearest’, ‘mirror’}, optional</span></dt><dd><p>How to handle values outside the image borders.</p>
</dd>
<dt><strong>cval</strong><span class="classifier">float, optional</span></dt><dd><p>Used in conjunction with mode ‘constant’, the value outside
the image boundaries.</p>
</dd>
<dt><strong>order</strong><span class="classifier">{‘rc’, ‘xy’}, optional</span></dt><dd><p>For 2D images, this parameter allows for the use of reverse or forward
order of the image axes in gradient computation. ‘rc’ indicates the use
of the first axis initially (Hrr, Hrc, Hcc), whilst ‘xy’ indicates the
usage of the last axis initially (Hxx, Hxy, Hyy). Images with higher
dimension must always use ‘rc’ order.</p>
</dd>
<dt><strong>use_gaussian_derivatives</strong><span class="classifier">boolean, optional</span></dt><dd><p>Indicates whether the Hessian is computed by convolving with Gaussian
derivatives, or by a simple finite-difference operation.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>H_elems</strong><span class="classifier">list of ndarray</span></dt><dd><p>Upper-diagonal elements of the hessian matrix for each pixel in the
input image. In 2D, this will be a three element list containing [Hrr,
Hrc, Hcc]. In nD, the list will contain <code class="docutils literal notranslate"><span class="pre">(n**2</span> <span class="pre">+</span> <span class="pre">n)</span> <span class="pre">/</span> <span class="pre">2</span></code> arrays.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The distributive property of derivatives and convolutions allows us to
restate the derivative of an image, I, smoothed with a Gaussian kernel, G,
as the convolution of the image with the derivative of G.</p>
<div class="math notranslate nohighlight">
\[\frac{\partial }{\partial x_i}(I * G) =
I * \left( \frac{\partial }{\partial x_i} G \right)\]</div>
<p>When <code class="docutils literal notranslate"><span class="pre">use_gaussian_derivatives</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this property is used to
compute the second order derivatives that make up the Hessian matrix.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">use_gaussian_derivatives</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, simple finite differences
on a Gaussian-smoothed image are used instead.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">hessian_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Hrr</span><span class="p">,</span> <span class="n">Hrc</span><span class="p">,</span> <span class="n">Hcc</span> <span class="o">=</span> <span class="n">hessian_matrix</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;rc&#39;</span><span class="p">,</span>
<span class="gp">... </span>                               <span class="n">use_gaussian_derivatives</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Hrc</span>
<span class="go">array([[ 0.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  0., -1.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0., -1.,  0.,  1.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  0.]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.hessian_matrix_det">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">hessian_matrix_det</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approximate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L341-L387"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.hessian_matrix_det" title="Link to this definition">#</a></dt>
<dd><p>Compute the approximate Hessian Determinant over an image.</p>
<p>The 2D approximate method uses box filters over integral images to
compute the approximate Hessian Determinant.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>The image over which to compute the Hessian Determinant.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation of the Gaussian kernel used for the Hessian
matrix.</p>
</dd>
<dt><strong>approximate</strong><span class="classifier">bool, optional</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the image is 2D, use a much faster approximate
computation. This argument has no effect on 3D and higher images.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>out</strong><span class="classifier">array</span></dt><dd><p>The array of the Determinant of Hessians.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For 2D images when <code class="docutils literal notranslate"><span class="pre">approximate=True</span></code>, the running time of this method
only depends on size of the image. It is independent of <code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma</span></code> as one
would expect. The downside is that the result for <code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma</span></code> less than <code class="xref py py-obj docutils literal notranslate"><span class="pre">3</span></code>
is not accurate, i.e., not similar to the result obtained if someone
computed the Hessian and took its determinant.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r48e33a732c34-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool,
“SURF: Speeded Up Robust Features”
<a class="reference external" href="ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf">ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf</a></p>
</div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.hessian_matrix_eigvals">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">hessian_matrix_eigvals</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">H_elems</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L489-L519"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.hessian_matrix_eigvals" title="Link to this definition">#</a></dt>
<dd><p>Compute eigenvalues of Hessian matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>H_elems</strong><span class="classifier">list of ndarray</span></dt><dd><p>The upper-diagonal elements of the Hessian matrix, as returned
by <a class="reference internal" href="#skimage.feature.hessian_matrix" title="skimage.feature.hessian_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hessian_matrix</span></code></a>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>eigs</strong><span class="classifier">ndarray</span></dt><dd><p>The eigenvalues of the Hessian matrix, in decreasing order. The
eigenvalues are the leading dimension. That is, <code class="docutils literal notranslate"><span class="pre">eigs[i,</span> <span class="pre">j,</span> <span class="pre">k]</span></code>
contains the ith-largest eigenvalue at position (j, k).</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">hessian_matrix</span><span class="p">,</span> <span class="n">hessian_matrix_eigvals</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">H_elems</span> <span class="o">=</span> <span class="n">hessian_matrix</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;rc&#39;</span><span class="p">,</span>
<span class="gp">... </span>                         <span class="n">use_gaussian_derivatives</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hessian_matrix_eigvals</span><span class="p">(</span><span class="n">H_elems</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">array([[ 0.,  0.,  2.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  0.,  1.,  0.],</span>
<span class="go">       [ 2.,  0., -2.,  0.,  2.],</span>
<span class="go">       [ 0.,  1.,  0.,  1.,  0.],</span>
<span class="go">       [ 0.,  0.,  2.,  0.,  0.]])</span>
</pre></div>
</div>
</dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.hog">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">hog</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">orientations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pixels_per_cell</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(8,</span> <span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cells_per_block</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(3,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'L2-Hys'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">visualize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform_sqrt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_vector</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_hog.py#L48-L341"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.hog" title="Link to this definition">#</a></dt>
<dd><p>Extract Histogram of Oriented Gradients (HOG) for a given image.</p>
<p>Compute a Histogram of Oriented Gradients (HOG) by</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>(optional) global image normalization</p></li>
<li><p>computing the gradient image in <code class="xref py py-obj docutils literal notranslate"><span class="pre">row</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">col</span></code></p></li>
<li><p>computing gradient histograms</p></li>
<li><p>normalizing across blocks</p></li>
<li><p>flattening into a feature vector</p></li>
</ol>
</div></blockquote>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">(M, N[, C]) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>orientations</strong><span class="classifier">int, optional</span></dt><dd><p>Number of orientation bins.</p>
</dd>
<dt><strong>pixels_per_cell</strong><span class="classifier">2-tuple (int, int), optional</span></dt><dd><p>Size (in pixels) of a cell.</p>
</dd>
<dt><strong>cells_per_block</strong><span class="classifier">2-tuple (int, int), optional</span></dt><dd><p>Number of cells in each block.</p>
</dd>
<dt><strong>block_norm</strong><span class="classifier">str {‘L1’, ‘L1-sqrt’, ‘L2’, ‘L2-Hys’}, optional</span></dt><dd><p>Block normalization method:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">L1</span></code></dt><dd><p>Normalization using L1-norm.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">L1-sqrt</span></code></dt><dd><p>Normalization using L1-norm, followed by square root.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">L2</span></code></dt><dd><p>Normalization using L2-norm.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">L2-Hys</span></code></dt><dd><p>Normalization using L2-norm, followed by limiting the
maximum values to 0.2 (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Hys</span></code> stands for <code class="xref py py-obj docutils literal notranslate"><span class="pre">hysteresis</span></code>) and
renormalization using L2-norm. (default)
For details, see <a class="reference internal" href="#ra159ccd8c91f-3" id="id44">[3]</a>, <a class="reference internal" href="#ra159ccd8c91f-4" id="id45">[4]</a>.</p>
</dd>
</dl>
</dd>
<dt><strong>visualize</strong><span class="classifier">bool, optional</span></dt><dd><p>Also return an image of the HOG.  For each cell and orientation bin,
the image contains a line segment that is centered at the cell center,
is perpendicular to the midpoint of the range of angles spanned by the
orientation bin, and has intensity proportional to the corresponding
histogram value.</p>
</dd>
<dt><strong>transform_sqrt</strong><span class="classifier">bool, optional</span></dt><dd><p>Apply power law compression to normalize the image before
processing. DO NOT use this if the image contains negative
values. Also see <code class="xref py py-obj docutils literal notranslate"><span class="pre">notes</span></code> section below.</p>
</dd>
<dt><strong>feature_vector</strong><span class="classifier">bool, optional</span></dt><dd><p>Return the data as a feature vector by calling .ravel() on the result
just before returning.</p>
</dd>
<dt><strong>channel_axis</strong><span class="classifier">int or None, optional</span></dt><dd><p>If None, the image is assumed to be a grayscale (single channel) image.
Otherwise, this parameter indicates which axis of the array corresponds
to channels.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.19: </span><code class="xref py py-obj docutils literal notranslate"><span class="pre">channel_axis</span></code> was added in 0.19.</p>
</div>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>out</strong><span class="classifier">(n_blocks_row, n_blocks_col, n_cells_row, n_cells_col, n_orient) ndarray</span></dt><dd><p>HOG descriptor for the image. If <code class="xref py py-obj docutils literal notranslate"><span class="pre">feature_vector</span></code> is True, a 1D
(flattened) array is returned.</p>
</dd>
<dt><strong>hog_image</strong><span class="classifier">(M, N) ndarray, optional</span></dt><dd><p>A visualisation of the HOG image. Only provided if <code class="xref py py-obj docutils literal notranslate"><span class="pre">visualize</span></code> is True.</p>
</dd>
</dl>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt>ValueError</dt><dd><p>If the image is too small given the values of pixels_per_cell and
cells_per_block.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The presented code implements the HOG extraction method from <a class="reference internal" href="#ra159ccd8c91f-2" id="id46">[2]</a> with
the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the
paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix
in the paper); (III) L1 block normalization is used (L2-Hys in the paper).</p>
<p>Power law compression, also known as Gamma correction, is used to reduce
the effects of shadowing and illumination variations. The compression makes
the dark regions lighter. When the kwarg <code class="xref py py-obj docutils literal notranslate"><span class="pre">transform_sqrt</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the function computes the square root of each color channel
and then applies the hog algorithm to the image.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra159ccd8c91f-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients</a></p>
</div>
<div class="citation" id="ra159ccd8c91f-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id46">2</a><span class="fn-bracket">]</span></span>
<p>Dalal, N and Triggs, B, Histograms of Oriented Gradients for
Human Detection, IEEE Computer Society Conference on Computer
Vision and Pattern Recognition 2005 San Diego, CA, USA,
<a class="reference external" href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf</a>,
<a class="reference external" href="https://doi.org/10.1109/CVPR.2005.177">DOI:10.1109/CVPR.2005.177</a></p>
</div>
<div class="citation" id="ra159ccd8c91f-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id44">3</a><span class="fn-bracket">]</span></span>
<p>Lowe, D.G., Distinctive image features from scale-invatiant
keypoints, International Journal of Computer Vision (2004) 60: 91,
<a class="reference external" href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a>,
<a class="reference external" href="https://doi.org/10.1023/B:VISI.0000029664.99615.94">DOI:10.1023/B:VISI.0000029664.99615.94</a></p>
</div>
<div class="citation" id="ra159ccd8c91f-4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">4</a><span class="fn-bracket">]</span></span>
<p>Dalal, N, Finding People in Images and Videos,
Human-Computer Interaction [cs.HC], Institut National Polytechnique
de Grenoble - INPG, 2006,
<a class="reference external" href="https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf">https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf</a></p>
</div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The Histogram of Oriented Gradient (HOG) feature descriptor is popular for object detection [1]_."><img alt="" src="../_images/sphx_glr_plot_hog_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py"><span class="std std-ref">Histogram of Oriented Gradients</span></a></p>
  <div class="sphx-glr-thumbnail-title">Histogram of Oriented Gradients</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.learn_gmm">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">learn_gmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">descriptors</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gm_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_fisher_vector.py#L38-L152"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.learn_gmm" title="Link to this definition">#</a></dt>
<dd><p>Estimate a Gaussian mixture model (GMM) given a set of descriptors and
number of modes (i.e. Gaussians). This function is essentially a wrapper
around the scikit-learn implementation of GMM, namely the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="(in scikit-learn v1.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.mixture.GaussianMixture</span></code></a> class.</p>
<p>Due to the nature of the Fisher vector, the only enforced parameter of the
underlying scikit-learn class is the covariance_type, which must be ‘diag’.</p>
<p>There is no simple way to know what value to use for <code class="xref py py-obj docutils literal notranslate"><span class="pre">n_modes</span></code> a-priori.
Typically, the value is usually one of <code class="docutils literal notranslate"><span class="pre">{16,</span> <span class="pre">32,</span> <span class="pre">64,</span> <span class="pre">128}</span></code>. One may train
a few GMMs and choose the one that maximises the log probability of the
GMM, or choose <code class="xref py py-obj docutils literal notranslate"><span class="pre">n_modes</span></code> such that the downstream classifier trained on
the resultant Fisher vectors has maximal performance.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>descriptors</strong><span class="classifier">np.ndarray (N, M) or list [(N1, M), (N2, M), …]</span></dt><dd><p>List of NumPy arrays, or a single NumPy array, of the descriptors
used to estimate the GMM. The reason a list of NumPy arrays is
permissible is because often when using a Fisher vector encoding,
descriptors/vectors are computed separately for each sample/image in
the dataset, such as SIFT vectors for each image. If a list if passed
in, then each element must be a NumPy array in which the number of
rows may differ (e.g. different number of SIFT vector for each image),
but the number of columns for each must be the same (i.e. the
dimensionality must be the same).</p>
</dd>
<dt><strong>n_modes</strong><span class="classifier">int</span></dt><dd><p>The number of modes/Gaussians to estimate during the GMM estimate.</p>
</dd>
<dt><strong>gm_args</strong><span class="classifier">dict</span></dt><dd><p>Keyword arguments that can be passed into the underlying scikit-learn
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="(in scikit-learn v1.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.mixture.GaussianMixture</span></code></a> class.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>gmm</strong><span class="classifier"><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="(in scikit-learn v1.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.mixture.GaussianMixture</span></code></a></span></dt><dd><p>The estimated GMM object, which contains the necessary parameters
needed to compute the Fisher vector.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rd6855c44e94e-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html">https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">fisher_vector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sift_for_images</span> <span class="o">=</span> <span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_modes</span> <span class="o">=</span> <span class="mi">16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Estimate 16-mode GMM with these synthetic SIFT vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gmm</span> <span class="o">=</span> <span class="n">learn_gmm</span><span class="p">(</span><span class="n">sift_for_images</span><span class="p">,</span> <span class="n">n_modes</span><span class="o">=</span><span class="n">num_modes</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A Fisher vector is an image feature encoding and quantization technique that can be seen as a soft or probabilistic version of the popular bag-of-visual-words or VLAD algorithms. Images are modelled using a visual vocabulary which is estimated using a K-mode Gaussian mixture model trained on low-level image features such as SIFT or ORB descriptors. The Fisher vector itself is a concatenation of the gradients of the Gaussian mixture model (GMM) with respect to its parameters - mixture weights, means, and covariance matrices."><img alt="" src="../_images/sphx_glr_plot_fisher_vector_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_fisher_vector.html#sphx-glr-auto-examples-features-detection-plot-fisher-vector-py"><span class="std std-ref">Fisher vector feature encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fisher vector feature encoding</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.local_binary_pattern">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">local_binary_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">R</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/texture.py#L318-L393"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.local_binary_pattern" title="Link to this definition">#</a></dt>
<dd><p>Compute the local binary patterns (LBP) of an image.</p>
<p>LBP is a visual descriptor often used in texture classification.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">(M, N) array</span></dt><dd><p>2D grayscale image.</p>
</dd>
<dt><strong>P</strong><span class="classifier">int</span></dt><dd><p>Number of circularly symmetric neighbor set points (quantization of
the angular space).</p>
</dd>
<dt><strong>R</strong><span class="classifier">float</span></dt><dd><p>Radius of circle (spatial resolution of the operator).</p>
</dd>
<dt><strong>method</strong><span class="classifier">str {‘default’, ‘ror’, ‘uniform’, ‘nri_uniform’, ‘var’}, optional</span></dt><dd><p>Method to determine the pattern:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">default</span></code></dt><dd><p>Original local binary pattern which is grayscale invariant but not
rotation invariant.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">ror</span></code></dt><dd><p>Extension of default pattern which is grayscale invariant and
rotation invariant.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">uniform</span></code></dt><dd><p>Uniform pattern which is grayscale invariant and rotation
invariant, offering finer quantization of the angular space.
For details, see <a class="reference internal" href="#r648eb9e75080-1" id="id52">[1]</a>.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">nri_uniform</span></code></dt><dd><p>Variant of uniform pattern which is grayscale invariant but not
rotation invariant. For details, see <a class="reference internal" href="#r648eb9e75080-2" id="id53">[2]</a> and <a class="reference internal" href="#r648eb9e75080-3" id="id54">[3]</a>.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">var</span></code></dt><dd><p>Variance of local image texture (related to contrast)
which is rotation invariant but not grayscale invariant.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">(M, N) array</span></dt><dd><p>LBP image.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r648eb9e75080-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id52">1</a><span class="fn-bracket">]</span></span>
<p>T. Ojala, M. Pietikainen, T. Maenpaa, “Multiresolution gray-scale
and rotation invariant texture classification with local binary
patterns”, IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, no. 7, pp. 971-987, July 2002
<a class="reference external" href="https://doi.org/10.1109/TPAMI.2002.1017623">DOI:10.1109/TPAMI.2002.1017623</a></p>
</div>
<div class="citation" id="r648eb9e75080-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id53">2</a><span class="fn-bracket">]</span></span>
<p>T. Ahonen, A. Hadid and M. Pietikainen. “Face recognition with
local binary patterns”, in Proc. Eighth European Conf. Computer
Vision, Prague, Czech Republic, May 11-14, 2004, pp. 469-481, 2004.
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851</a>
<a class="reference external" href="https://doi.org/10.1007/978-3-540-24670-1_36">DOI:10.1007/978-3-540-24670-1_36</a></p>
</div>
<div class="citation" id="r648eb9e75080-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id54">3</a><span class="fn-bracket">]</span></span>
<p>T. Ahonen, A. Hadid and M. Pietikainen, “Face Description with
Local Binary Patterns: Application to Face Recognition”,
IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 28, no. 12, pp. 2037-2041, Dec. 2006
<a class="reference external" href="https://doi.org/10.1109/TPAMI.2006.244">DOI:10.1109/TPAMI.2006.244</a></p>
</div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this example, we will see how to classify textures based on LBP (Local Binary Pattern). LBP looks at points surrounding a central point and tests whether the surrounding points are greater than or less than the central point (i.e. gives a binary result)."><img alt="" src="../_images/sphx_glr_plot_local_binary_pattern_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_local_binary_pattern.html#sphx-glr-auto-examples-features-detection-plot-local-binary-pattern-py"><span class="std std-ref">Local Binary Pattern for texture classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Local Binary Pattern for texture classification</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.match_descriptors">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">match_descriptors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">descriptors1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">descriptors2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_check</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/match.py#L5-L103"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.match_descriptors" title="Link to this definition">#</a></dt>
<dd><p>Brute-force matching of descriptors.</p>
<p>For each descriptor in the first set this matcher finds the closest
descriptor in the second set (and vice-versa in the case of enabled
cross-checking).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>descriptors1</strong><span class="classifier">(M, P) array</span></dt><dd><p>Descriptors of size P about M keypoints in the first image.</p>
</dd>
<dt><strong>descriptors2</strong><span class="classifier">(N, P) array</span></dt><dd><p>Descriptors of size P about N keypoints in the second image.</p>
</dd>
<dt><strong>metric</strong><span class="classifier">{‘euclidean’, ‘cityblock’, ‘minkowski’, ‘hamming’, …} , optional</span></dt><dd><p>The metric to compute the distance between two descriptors. See
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist" title="(in SciPy v1.14.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.spatial.distance.cdist</span></code></a> for all possible types. The hamming
distance should be used for binary descriptors. By default the L2-norm
is used for all descriptors of dtype float or double and the Hamming
distance is used for binary descriptors automatically.</p>
</dd>
<dt><strong>p</strong><span class="classifier">int, optional</span></dt><dd><p>The p-norm to apply for <code class="docutils literal notranslate"><span class="pre">metric='minkowski'</span></code>.</p>
</dd>
<dt><strong>max_distance</strong><span class="classifier">float, optional</span></dt><dd><p>Maximum allowed distance between descriptors of two keypoints
in separate images to be regarded as a match.</p>
</dd>
<dt><strong>cross_check</strong><span class="classifier">bool, optional</span></dt><dd><p>If True, the matched keypoints are returned after cross checking i.e. a
matched pair (keypoint1, keypoint2) is returned if keypoint2 is the
best match for keypoint1 in second image and keypoint1 is the best
match for keypoint2 in first image.</p>
</dd>
<dt><strong>max_ratio</strong><span class="classifier">float, optional</span></dt><dd><p>Maximum ratio of distances between first and second closest descriptor
in the second set of descriptors. This threshold is useful to filter
ambiguous matches between the two descriptor sets. The choice of this
value depends on the statistics of the chosen descriptor, e.g.,
for SIFT descriptors a value of 0.8 is usually chosen, see
D.G. Lowe, “Distinctive Image Features from Scale-Invariant Keypoints”,
International Journal of Computer Vision, 2004.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>matches</strong><span class="classifier">(Q, 2) array</span></dt><dd><p>Indices of corresponding matches in first and second set of
descriptors, where <code class="docutils literal notranslate"><span class="pre">matches[:,</span> <span class="pre">0]</span></code> denote the indices in the first
and <code class="docutils literal notranslate"><span class="pre">matches[:,</span> <span class="pre">1]</span></code> the indices in the second set of descriptors.</p>
</dd>
</dl>
</dd>
</dl>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to robustly estimate epipolar geometry &lt;https://en.wikipedia.org/wiki/Epipolar_geometry&gt; (the geometry of stereo vision) between two views using sparse ORB feature correspondences."><img alt="" src="../_images/sphx_glr_plot_fundamental_matrix_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_fundamental_matrix.html#sphx-glr-auto-examples-transform-plot-fundamental-matrix-py"><span class="std std-ref">Fundamental matrix estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fundamental matrix estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the ORB feature detection and binary description algorithm. It uses an oriented FAST detection method and the rotated BRIEF descriptors."><img alt="" src="../_images/sphx_glr_plot_orb_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_orb.html#sphx-glr-auto-examples-features-detection-plot-orb-py"><span class="std std-ref">ORB feature detector and binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">ORB feature detector and binary descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the BRIEF binary description algorithm. The descriptor consists of relatively few bits and can be computed using a set of intensity difference tests. The short binary descriptor results in low memory footprint and very efficient matching based on the Hamming distance metric. BRIEF does not provide rotation-invariance. Scale-invariance can be achieved by detecting and extracting features at different scales."><img alt="" src="../_images/sphx_glr_plot_brief_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_brief.html#sphx-glr-auto-examples-features-detection-plot-brief-py"><span class="std std-ref">BRIEF binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">BRIEF binary descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the SIFT feature detection and its description algorithm."><img alt="" src="../_images/sphx_glr_plot_sift_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_sift.html#sphx-glr-auto-examples-features-detection-plot-sift-py"><span class="std std-ref">SIFT feature detector and descriptor extractor</span></a></p>
  <div class="sphx-glr-thumbnail-title">SIFT feature detector and descriptor extractor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.match_template">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">match_template</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">template</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constant_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/template.py#L33-L186"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.match_template" title="Link to this definition">#</a></dt>
<dd><p>Match a template to a 2-D or 3-D image using normalized correlation.</p>
<p>The output is an array with values between -1.0 and 1.0. The value at a
given position corresponds to the correlation coefficient between the image
and the template.</p>
<p>For <code class="xref py py-obj docutils literal notranslate"><span class="pre">pad_input=True</span></code> matches correspond to the center and otherwise to the
top-left corner of the template. To find the best match you must search for
peaks in the response (output) image.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">(M, N[, P]) array</span></dt><dd><p>2-D or 3-D input image.</p>
</dd>
<dt><strong>template</strong><span class="classifier">(m, n[, p]) array</span></dt><dd><p>Template to locate. It must be <code class="xref py py-obj docutils literal notranslate"><span class="pre">(m</span> <span class="pre">&lt;=</span> <span class="pre">M,</span> <span class="pre">n</span> <span class="pre">&lt;=</span> <span class="pre">N[,</span> <span class="pre">p</span> <span class="pre">&lt;=</span> <span class="pre">P])</span></code>.</p>
</dd>
<dt><strong>pad_input</strong><span class="classifier">bool</span></dt><dd><p>If True, pad <code class="xref py py-obj docutils literal notranslate"><span class="pre">image</span></code> so that output is the same size as the image, and
output values correspond to the template center. Otherwise, the output
is an array with shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(M</span> <span class="pre">-</span> <span class="pre">m</span> <span class="pre">+</span> <span class="pre">1,</span> <span class="pre">N</span> <span class="pre">-</span> <span class="pre">n</span> <span class="pre">+</span> <span class="pre">1)</span></code> for an <code class="xref py py-obj docutils literal notranslate"><span class="pre">(M,</span> <span class="pre">N)</span></code> image
and an <code class="xref py py-obj docutils literal notranslate"><span class="pre">(m,</span> <span class="pre">n)</span></code> template, and matches correspond to origin
(top-left corner) of the template.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">see <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.pad.html#numpy.pad" title="(in NumPy v2.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.pad</span></code></a>, optional</span></dt><dd><p>Padding mode.</p>
</dd>
<dt><strong>constant_values</strong><span class="classifier">see <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.pad.html#numpy.pad" title="(in NumPy v2.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.pad</span></code></a>, optional</span></dt><dd><p>Constant values used in conjunction with <code class="docutils literal notranslate"><span class="pre">mode='constant'</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">array</span></dt><dd><p>Response image with correlation coefficients.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Details on the cross-correlation are presented in <a class="reference internal" href="#r7bfca17c0278-1" id="id58">[1]</a>. This implementation
uses FFT convolutions of the image and the template. Reference <a class="reference internal" href="#r7bfca17c0278-2" id="id59">[2]</a>
presents similar derivations but the approximation presented in this
reference is not used in our implementation.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r7bfca17c0278-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id58">1</a><span class="fn-bracket">]</span></span>
<p>J. P. Lewis, “Fast Normalized Cross-Correlation”, Industrial Light
and Magic.</p>
</div>
<div class="citation" id="r7bfca17c0278-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id59">2</a><span class="fn-bracket">]</span></span>
<p>Briechle and Hanebeck, “Template Matching using Fast Normalized
Cross Correlation”, Proceedings of the SPIE (2001).
<a class="reference external" href="https://doi.org/10.1117/12.421129">DOI:10.1117/12.421129</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">template</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">template</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">template</span>
<span class="go">array([[0., 0., 0.],</span>
<span class="go">       [0., 1., 0.],</span>
<span class="go">       [0., 0., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span>
<span class="go">array([[ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  1.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  0.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0., -1.,  0.],</span>
<span class="go">       [ 0.,  0.,  0.,  0.,  0.,  0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">match_template</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">template</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">array([[ 1.   , -0.125,  0.   ,  0.   ],</span>
<span class="go">       [-0.125, -0.125,  0.   ,  0.   ],</span>
<span class="go">       [ 0.   ,  0.   ,  0.125,  0.125],</span>
<span class="go">       [ 0.   ,  0.   ,  0.125, -1.   ]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">match_template</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">template</span><span class="p">,</span> <span class="n">pad_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">array([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],</span>
<span class="go">       [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],</span>
<span class="go">       [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],</span>
<span class="go">       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],</span>
<span class="go">       [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],</span>
<span class="go">       [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="We use template matching to identify the occurrence of an image patch (in this case, a sub-image centered on a single coin). Here, we return a single match (the exact same coin), so the maximum value in the match_template result corresponds to the coin location. The other coins look similar, and thus have local maxima; if you expect multiple matches, you should use a proper peak-finding function."><img alt="" src="../_images/sphx_glr_plot_template_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_template.html#sphx-glr-auto-examples-features-detection-plot-template-py"><span class="std std-ref">Template Matching</span></a></p>
  <div class="sphx-glr-thumbnail-title">Template Matching</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.multiblock_lbp">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">multiblock_lbp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">int_image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">width</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">height</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/texture.py#L396-L440"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.multiblock_lbp" title="Link to this definition">#</a></dt>
<dd><p>Multi-block local binary pattern (MB-LBP).</p>
<p>The features are calculated similarly to local binary patterns (LBPs),
(See <a class="reference internal" href="#skimage.feature.local_binary_pattern" title="skimage.feature.local_binary_pattern"><code class="xref py py-meth docutils literal notranslate"><span class="pre">local_binary_pattern()</span></code></a>) except that summed blocks are
used instead of individual pixel values.</p>
<p>MB-LBP is an extension of LBP that can be computed on multiple scales
in constant time using the integral image. Nine equally-sized rectangles
are used to compute a feature. For each rectangle, the sum of the pixel
intensities is computed. Comparisons of these sums to that of the central
rectangle determine the feature, similarly to LBP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>int_image</strong><span class="classifier">(N, M) array</span></dt><dd><p>Integral image.</p>
</dd>
<dt><strong>r</strong><span class="classifier">int</span></dt><dd><p>Row-coordinate of top left corner of a rectangle containing feature.</p>
</dd>
<dt><strong>c</strong><span class="classifier">int</span></dt><dd><p>Column-coordinate of top left corner of a rectangle containing feature.</p>
</dd>
<dt><strong>width</strong><span class="classifier">int</span></dt><dd><p>Width of one of the 9 equal rectangles that will be used to compute
a feature.</p>
</dd>
<dt><strong>height</strong><span class="classifier">int</span></dt><dd><p>Height of one of the 9 equal rectangles that will be used to compute
a feature.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">int</span></dt><dd><p>8-bit MB-LBP feature descriptor.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra36744213751-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>L. Zhang, R. Chu, S. Xiang, S. Liao, S.Z. Li. “Face Detection Based
on Multi-Block LBP Representation”, In Proceedings: Advances in
Biometrics, International Conference, ICB 2007, Seoul, Korea.
<a class="reference external" href="http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf">http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf</a>
<a class="reference external" href="https://doi.org/10.1007/978-3-540-74549-5_2">DOI:10.1007/978-3-540-74549-5_2</a></p>
</div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example shows how to compute multi-block local binary pattern (MB-LBP) features as well as how to visualize them."><img alt="" src="../_images/sphx_glr_plot_multiblock_local_binary_pattern_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_multiblock_local_binary_pattern.html#sphx-glr-auto-examples-features-detection-plot-multiblock-local-binary-pattern-py"><span class="std std-ref">Multi-Block Local Binary Pattern for texture classification</span></a></p>
  <div class="sphx-glr-thumbnail-title">Multi-Block Local Binary Pattern for texture classification</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.multiscale_basic_features">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">multiscale_basic_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intensity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edges</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texture</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/_basic_features.py#L115-L198"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.multiscale_basic_features" title="Link to this definition">#</a></dt>
<dd><p>Local features for a single- or multi-channel nd image.</p>
<p>Intensity, gradient intensity and local structure are computed at
different scales thanks to Gaussian blurring.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input image, which can be grayscale or multichannel.</p>
</dd>
<dt><strong>intensity</strong><span class="classifier">bool, default True</span></dt><dd><p>If True, pixel intensities averaged over the different scales
are added to the feature set.</p>
</dd>
<dt><strong>edges</strong><span class="classifier">bool, default True</span></dt><dd><p>If True, intensities of local gradients averaged over the different
scales are added to the feature set.</p>
</dd>
<dt><strong>texture</strong><span class="classifier">bool, default True</span></dt><dd><p>If True, eigenvalues of the Hessian matrix after Gaussian blurring
at different scales are added to the feature set.</p>
</dd>
<dt><strong>sigma_min</strong><span class="classifier">float, optional</span></dt><dd><p>Smallest value of the Gaussian kernel used to average local
neighborhoods before extracting features.</p>
</dd>
<dt><strong>sigma_max</strong><span class="classifier">float, optional</span></dt><dd><p>Largest value of the Gaussian kernel used to average local
neighborhoods before extracting features.</p>
</dd>
<dt><strong>num_sigma</strong><span class="classifier">int, optional</span></dt><dd><p>Number of values of the Gaussian kernel between sigma_min and sigma_max.
If None, sigma_min multiplied by powers of 2 are used.</p>
</dd>
<dt><strong>num_workers</strong><span class="classifier">int or None, optional</span></dt><dd><p>The number of parallel threads to use. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, the full
set of available cores are used.</p>
</dd>
<dt><strong>channel_axis</strong><span class="classifier">int or None, optional</span></dt><dd><p>If None, the image is assumed to be a grayscale (single channel) image.
Otherwise, this parameter indicates which axis of the array corresponds
to channels.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.19: </span><code class="docutils literal notranslate"><span class="pre">channel_axis</span></code> was added in 0.19.</p>
</div>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>features</strong><span class="classifier">np.ndarray</span></dt><dd><p>Array of shape <code class="docutils literal notranslate"><span class="pre">image.shape</span> <span class="pre">+</span> <span class="pre">(n_features,)</span></code>. When <code class="xref py py-obj docutils literal notranslate"><span class="pre">channel_axis</span></code> is
not None, all channels are concatenated along the features dimension.
(i.e. <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">==</span> <span class="pre">n_features_singlechannel</span> <span class="pre">*</span> <span class="pre">n_channels</span></code>)</p>
</dd>
</dl>
</dd>
</dl>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="A pixel-based segmentation is computed here using local features based on local intensity, edges and textures at different scales. A user-provided mask is used to identify different regions. The pixels of the mask are used to train a random-forest classifier [1]_ from scikit-learn. Unlabeled pixels are then labeled from the prediction of the classifier."><img alt="" src="../_images/sphx_glr_plot_trainable_segmentation_thumb.png" />
<p><a class="reference internal" href="../auto_examples/segmentation/plot_trainable_segmentation.html#sphx-glr-auto-examples-segmentation-plot-trainable-segmentation-py"><span class="std std-ref">Trainable segmentation using local features and random forests</span></a></p>
  <div class="sphx-glr-thumbnail-title">Trainable segmentation using local features and random forests</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.peak_local_max">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">peak_local_max</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_abs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_rel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exclude_border</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_peaks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">footprint</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_peaks_per_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/peak.py#L115-L310"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.peak_local_max" title="Link to this definition">#</a></dt>
<dd><p>Find peaks in an image as coordinate list.</p>
<p>Peaks are the local maxima in a region of <code class="xref py py-obj docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">min_distance</span> <span class="pre">+</span> <span class="pre">1</span></code>
(i.e. peaks are separated by at least <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_distance</span></code>).</p>
<p>If both <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_abs</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold_rel</span></code> are provided, the maximum
of the two is chosen as the minimum intensity threshold of peaks.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Prior to version 0.18, peaks of the same height within a radius of
<code class="xref py py-obj docutils literal notranslate"><span class="pre">min_distance</span></code> were all returned, but this could cause unexpected
behaviour. From 0.18 onwards, an arbitrary peak within the region is
returned. See issue gh-2592.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>min_distance</strong><span class="classifier">int, optional</span></dt><dd><p>The minimal allowed distance separating peaks. To find the
maximum number of peaks, use <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_distance=1</span></code>.</p>
</dd>
<dt><strong>threshold_abs</strong><span class="classifier">float or None, optional</span></dt><dd><p>Minimum intensity of peaks. By default, the absolute threshold is
the minimum intensity of the image.</p>
</dd>
<dt><strong>threshold_rel</strong><span class="classifier">float or None, optional</span></dt><dd><p>Minimum intensity of peaks, calculated as
<code class="docutils literal notranslate"><span class="pre">max(image)</span> <span class="pre">*</span> <span class="pre">threshold_rel</span></code>.</p>
</dd>
<dt><strong>exclude_border</strong><span class="classifier">int, tuple of ints, or bool, optional</span></dt><dd><p>If positive integer, <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code> excludes peaks from within
<code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image.
If tuple of non-negative ints, the length of the tuple must match the
input array’s dimensionality.  Each element of the tuple will exclude
peaks from within <code class="xref py py-obj docutils literal notranslate"><span class="pre">exclude_border</span></code>-pixels of the border of the image
along that dimension.
If True, takes the <code class="xref py py-obj docutils literal notranslate"><span class="pre">min_distance</span></code> parameter as value.
If zero or False, peaks are identified regardless of their distance
from the border.</p>
</dd>
<dt><strong>num_peaks</strong><span class="classifier">int, optional</span></dt><dd><p>Maximum number of peaks. When the number of peaks exceeds <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_peaks</span></code>,
return <code class="xref py py-obj docutils literal notranslate"><span class="pre">num_peaks</span></code> peaks based on highest peak intensity.</p>
</dd>
<dt><strong>footprint</strong><span class="classifier">ndarray of bools, optional</span></dt><dd><p>If provided, <code class="xref py py-obj docutils literal notranslate"><span class="pre">footprint</span> <span class="pre">==</span> <span class="pre">1</span></code> represents the local region within which
to search for peaks at every point in <code class="xref py py-obj docutils literal notranslate"><span class="pre">image</span></code>.</p>
</dd>
<dt><strong>labels</strong><span class="classifier">ndarray of ints, optional</span></dt><dd><p>If provided, each unique region <code class="xref py py-obj docutils literal notranslate"><span class="pre">labels</span> <span class="pre">==</span> <span class="pre">value</span></code> represents a unique
region to search for peaks. Zero is reserved for background.</p>
</dd>
<dt><strong>num_peaks_per_label</strong><span class="classifier">int, optional</span></dt><dd><p>Maximum number of peaks for each label.</p>
</dd>
<dt><strong>p_norm</strong><span class="classifier">float</span></dt><dd><p>Which Minkowski p-norm to use. Should be in the range [1, inf].
A finite large p may cause a ValueError if overflow can occur.
<code class="docutils literal notranslate"><span class="pre">inf</span></code> corresponds to the Chebyshev distance and 2 to the
Euclidean distance.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">ndarray</span></dt><dd><p>The coordinates of the peaks.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#skimage.feature.corner_peaks" title="skimage.feature.corner_peaks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.corner_peaks</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The peak local maximum function returns the coordinates of local peaks
(maxima) in an image. Internally, a maximum filter is used for finding
local maxima. This operation dilates the original image. After comparison
of the dilated and original images, this function returns the coordinates
of the peaks where the dilated image equals the original image.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span>
<span class="go">array([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 1.5, 0. , 1. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 0. , 0. , 0. , 0. , 0. ],</span>
<span class="go">       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">peak_local_max</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[3, 2],</span>
<span class="go">       [3, 4]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">peak_local_max</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">array([[3, 2]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">peak_idx</span> <span class="o">=</span> <span class="n">peak_local_max</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="n">exclude_border</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">peak_idx</span>
<span class="go">array([[10, 10, 10],</span>
<span class="go">       [15, 15, 15]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">peak_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">peak_mask</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">peak_idx</span><span class="o">.</span><span class="n">T</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">peak_mask</span><span class="p">)</span>
<span class="go">array([[10, 10, 10],</span>
<span class="go">       [15, 15, 15]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The peak_local_max function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image and merges neighboring local maxima closer than the size of the dilation. Locations where the original image is equal to the dilated image are returned as local maxima."><img alt="" src="../_images/sphx_glr_plot_peak_local_max_thumb.png" />
<p><a class="reference internal" href="../auto_examples/segmentation/plot_peak_local_max.html#sphx-glr-auto-examples-segmentation-plot-peak-local-max-py"><span class="std std-ref">Finding local maxima</span></a></p>
  <div class="sphx-glr-thumbnail-title">Finding local maxima</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The watershed is a classical algorithm used for segmentation, that is, for separating different objects in an image."><img alt="" src="../_images/sphx_glr_plot_watershed_thumb.png" />
<p><a class="reference internal" href="../auto_examples/segmentation/plot_watershed.html#sphx-glr-auto-examples-segmentation-plot-watershed-py"><span class="std std-ref">Watershed segmentation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Watershed segmentation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this example, we analyze a microscopy image of human cells. We use data provided by Jason Moffat [1]_ through CellProfiler."><img alt="" src="../_images/sphx_glr_plot_human_mitosis_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_human_mitosis.html#sphx-glr-auto-examples-applications-plot-human-mitosis-py"><span class="std std-ref">Segment human cells (in mitosis)</span></a></p>
  <div class="sphx-glr-thumbnail-title">Segment human cells (in mitosis)</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.plot_matched_features">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">plot_matched_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matches</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matches_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">only_matches</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alignment</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'horizontal'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/util.py#L44-L164"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.plot_matched_features" title="Link to this definition">#</a></dt>
<dd><p>Plot matched features between two images.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image0</strong><span class="classifier">(N, M [, 3]) array</span></dt><dd><p>First image.</p>
</dd>
<dt><strong>image1</strong><span class="classifier">(N, M [, 3]) array</span></dt><dd><p>Second image.</p>
</dd>
<dt><strong>keypoints0</strong><span class="classifier">(K1, 2) array</span></dt><dd><p>First keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>keypoints1</strong><span class="classifier">(K2, 2) array</span></dt><dd><p>Second keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>matches</strong><span class="classifier">(Q, 2) array</span></dt><dd><p>Indices of corresponding matches in first and second sets of
descriptors, where <code class="xref py py-obj docutils literal notranslate"><span class="pre">matches[:,</span> <span class="pre">0]</span></code> (resp. <code class="xref py py-obj docutils literal notranslate"><span class="pre">matches[:,</span> <span class="pre">1]</span></code>) contains
the indices in the first (resp. second) set of descriptors.</p>
</dd>
<dt><strong>ax</strong><span class="classifier">matplotlib.axes.Axes</span></dt><dd><p>The Axes object where the images and their matched features are drawn.</p>
</dd>
<dt><strong>keypoints_color</strong><span class="classifier">matplotlib color, optional</span></dt><dd><p>Color for keypoint locations.</p>
</dd>
<dt><strong>matches_color</strong><span class="classifier">matplotlib color, optional</span></dt><dd><p>Color for lines which connect keypoint matches. By default the
color is chosen randomly.</p>
</dd>
<dt><strong>only_matches</strong><span class="classifier">bool, optional</span></dt><dd><p>Set to True to plot matches only and not the keypoint locations.</p>
</dd>
<dt><strong>alignment</strong><span class="classifier">{‘horizontal’, ‘vertical’}, optional</span></dt><dd><p>Whether to show the two images side by side (<code class="xref py py-obj docutils literal notranslate"><span class="pre">'horizontal'</span></code>), or one above
the other (<code class="xref py py-obj docutils literal notranslate"><span class="pre">'vertical'</span></code>).</p>
</dd>
</dl>
</dd>
</dl>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to robustly estimate epipolar geometry &lt;https://en.wikipedia.org/wiki/Epipolar_geometry&gt; (the geometry of stereo vision) between two views using sparse ORB feature correspondences."><img alt="" src="../_images/sphx_glr_plot_fundamental_matrix_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_fundamental_matrix.html#sphx-glr-auto-examples-transform-plot-fundamental-matrix-py"><span class="std std-ref">Fundamental matrix estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fundamental matrix estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="In this simplified example we first generate two synthetic images as if they were taken from different view points."><img alt="" src="../_images/sphx_glr_plot_matching_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_matching.html#sphx-glr-auto-examples-transform-plot-matching-py"><span class="std std-ref">Robust matching using RANSAC</span></a></p>
  <div class="sphx-glr-thumbnail-title">Robust matching using RANSAC</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the ORB feature detection and binary description algorithm. It uses an oriented FAST detection method and the rotated BRIEF descriptors."><img alt="" src="../_images/sphx_glr_plot_orb_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_orb.html#sphx-glr-auto-examples-features-detection-plot-orb-py"><span class="std std-ref">ORB feature detector and binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">ORB feature detector and binary descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the BRIEF binary description algorithm. The descriptor consists of relatively few bits and can be computed using a set of intensity difference tests. The short binary descriptor results in low memory footprint and very efficient matching based on the Hamming distance metric. BRIEF does not provide rotation-invariance. Scale-invariance can be achieved by detecting and extracting features at different scales."><img alt="" src="../_images/sphx_glr_plot_brief_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_brief.html#sphx-glr-auto-examples-features-detection-plot-brief-py"><span class="std std-ref">BRIEF binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">BRIEF binary descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the SIFT feature detection and its description algorithm."><img alt="" src="../_images/sphx_glr_plot_sift_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_sift.html#sphx-glr-auto-examples-features-detection-plot-sift-py"><span class="std std-ref">SIFT feature detector and descriptor extractor</span></a></p>
  <div class="sphx-glr-thumbnail-title">SIFT feature detector and descriptor extractor</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.shape_index">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">shape_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L522-L600"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.shape_index" title="Link to this definition">#</a></dt>
<dd><p>Compute the shape index.</p>
<p>The shape index, as defined by Koenderink &amp; van Doorn <a class="reference internal" href="#rc8faae48965f-1" id="id63">[1]</a>, is a
single valued measure of local curvature, assuming the image as a 3D plane
with intensities representing heights.</p>
<p>It is derived from the eigenvalues of the Hessian, and its
value ranges from -1 to 1 (and is undefined (=NaN) in <em>flat</em> regions),
with following ranges representing following shapes:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id82">
<caption><span class="caption-text">Ranges of the shape index and corresponding shapes.</span><a class="headerlink" href="#id82" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Interval (s in …)</p></th>
<th class="head"><p>Shape</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[  -1, -7/8)</p></td>
<td><p>Spherical cup</p></td>
</tr>
<tr class="row-odd"><td><p>[-7/8, -5/8)</p></td>
<td><p>Through</p></td>
</tr>
<tr class="row-even"><td><p>[-5/8, -3/8)</p></td>
<td><p>Rut</p></td>
</tr>
<tr class="row-odd"><td><p>[-3/8, -1/8)</p></td>
<td><p>Saddle rut</p></td>
</tr>
<tr class="row-even"><td><p>[-1/8, +1/8)</p></td>
<td><p>Saddle</p></td>
</tr>
<tr class="row-odd"><td><p>[+1/8, +3/8)</p></td>
<td><p>Saddle ridge</p></td>
</tr>
<tr class="row-even"><td><p>[+3/8, +5/8)</p></td>
<td><p>Ridge</p></td>
</tr>
<tr class="row-odd"><td><p>[+5/8, +7/8)</p></td>
<td><p>Dome</p></td>
</tr>
<tr class="row-even"><td><p>[+7/8,   +1]</p></td>
<td><p>Spherical cap</p></td>
</tr>
</tbody>
</table>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">(M, N) ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used for
smoothing the input data before Hessian eigen value calculation.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">{‘constant’, ‘reflect’, ‘wrap’, ‘nearest’, ‘mirror’}, optional</span></dt><dd><p>How to handle values outside the image borders</p>
</dd>
<dt><strong>cval</strong><span class="classifier">float, optional</span></dt><dd><p>Used in conjunction with mode ‘constant’, the value outside
the image boundaries.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>s</strong><span class="classifier">ndarray</span></dt><dd><p>Shape index</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc8faae48965f-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id63">1</a><span class="fn-bracket">]</span></span>
<p>Koenderink, J. J. &amp; van Doorn, A. J.,
“Surface shape and curvature scales”,
Image and Vision Computing, 1992, 10, 557-564.
<a class="reference external" href="https://doi.org/10.1016/0262-8856(92)90076-F">DOI:10.1016/0262-8856(92)90076-F</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">shape_index</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">shape_index</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span>
<span class="go">array([[ nan,  nan, -0.5,  nan,  nan],</span>
<span class="go">       [ nan, -0. ,  nan, -0. ,  nan],</span>
<span class="go">       [-0.5,  nan, -1. ,  nan, -0.5],</span>
<span class="go">       [ nan, -0. ,  nan, -0. ,  nan],</span>
<span class="go">       [ nan,  nan, -0.5,  nan,  nan]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The shape index is a single valued measure of local curvature, derived from the eigen values of the Hessian, defined by Koenderink &amp; van Doorn [1]_."><img alt="" src="../_images/sphx_glr_plot_shape_index_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_shape_index.html#sphx-glr-auto-examples-features-detection-plot-shape-index-py"><span class="std std-ref">Shape Index</span></a></p>
  <div class="sphx-glr-thumbnail-title">Shape Index</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.structure_tensor">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">structure_tensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cval</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rc'</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L46-L131"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.structure_tensor" title="Link to this definition">#</a></dt>
<dd><p>Compute structure tensor using sum of squared differences.</p>
<p>The (2-dimensional) structure tensor A is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">Arr</span> <span class="n">Arc</span><span class="p">]</span>
    <span class="p">[</span><span class="n">Arc</span> <span class="n">Acc</span><span class="p">]</span>
</pre></div>
</div>
<p>which is approximated by the weighted sum of squared differences in a local
window around each pixel in the image. This formula can be extended to a
larger number of dimensions (see <a class="reference internal" href="#r0bb93eb224ab-1" id="id65">[1]</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">ndarray</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>sigma</strong><span class="classifier">float or array-like of float, optional</span></dt><dd><p>Standard deviation used for the Gaussian kernel, which is used as a
weighting function for the local summation of squared differences.
If sigma is an iterable, its length must be equal to <code class="xref py py-obj docutils literal notranslate"><span class="pre">image.ndim</span></code> and
each element is used for the Gaussian kernel applied along its
respective axis.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">{‘constant’, ‘reflect’, ‘wrap’, ‘nearest’, ‘mirror’}, optional</span></dt><dd><p>How to handle values outside the image borders.</p>
</dd>
<dt><strong>cval</strong><span class="classifier">float, optional</span></dt><dd><p>Used in conjunction with mode ‘constant’, the value outside
the image boundaries.</p>
</dd>
<dt><strong>order</strong><span class="classifier">{‘rc’, ‘xy’}, optional</span></dt><dd><p>NOTE: ‘xy’ is only an option for 2D images, higher dimensions must
always use ‘rc’ order. This parameter allows for the use of reverse or
forward order of the image axes in gradient computation. ‘rc’ indicates
the use of the first axis initially (Arr, Arc, Acc), whilst ‘xy’
indicates the usage of the last axis initially (Axx, Axy, Ayy).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>A_elems</strong><span class="classifier">list of ndarray</span></dt><dd><p>Upper-diagonal elements of the structure tensor for each pixel in the
input image.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#skimage.feature.structure_tensor_eigenvalues" title="skimage.feature.structure_tensor_eigenvalues"><code class="xref py py-obj docutils literal notranslate"><span class="pre">structure_tensor_eigenvalues</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r0bb93eb224ab-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id65">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Structure_tensor">https://en.wikipedia.org/wiki/Structure_tensor</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">structure_tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Arr</span><span class="p">,</span> <span class="n">Arc</span><span class="p">,</span> <span class="n">Acc</span> <span class="o">=</span> <span class="n">structure_tensor</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;rc&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Acc</span>
<span class="go">array([[0., 0., 0., 0., 0.],</span>
<span class="go">       [0., 1., 0., 1., 0.],</span>
<span class="go">       [0., 4., 0., 4., 0.],</span>
<span class="go">       [0., 1., 0., 1., 0.],</span>
<span class="go">       [0., 0., 0., 0., 0.]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, we compute the structure tensor of a 3D image. For a general introduction to 3D image processing, please refer to sphx_glr_auto_examples_applications_plot_3d_image_processing.py. The data we use here are sampled from an image of kidney tissue obtained by confocal fluorescence microscopy (more details at [1]_ under kidney-tissue-fluorescence.tif)."><img alt="" src="../_images/sphx_glr_plot_3d_structure_tensor_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_3d_structure_tensor.html#sphx-glr-auto-examples-applications-plot-3d-structure-tensor-py"><span class="std std-ref">Estimate anisotropy in a 3D microscopy image</span></a></p>
  <div class="sphx-glr-thumbnail-title">Estimate anisotropy in a 3D microscopy image</div>
</div></div></dd></dl>

<hr class="docutils" />
<dl class="py function">
<dt class="sig sig-object py" id="skimage.feature.structure_tensor_eigenvalues">
<span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">structure_tensor_eigenvalues</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">A_elems</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/corner.py#L452-L486"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.structure_tensor_eigenvalues" title="Link to this definition">#</a></dt>
<dd><p>Compute eigenvalues of structure tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>A_elems</strong><span class="classifier">list of ndarray</span></dt><dd><p>The upper-diagonal elements of the structure tensor, as returned
by <a class="reference internal" href="#skimage.feature.structure_tensor" title="skimage.feature.structure_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">structure_tensor</span></code></a>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray</dt><dd><p>The eigenvalues of the structure tensor, in decreasing order. The
eigenvalues are the leading dimension. That is, the coordinate
[i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#skimage.feature.structure_tensor" title="skimage.feature.structure_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">structure_tensor</span></code></a></dt><dd></dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">structure_tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">structure_tensor_eigenvalues</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A_elems</span> <span class="o">=</span> <span class="n">structure_tensor</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;rc&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">structure_tensor_eigenvalues</span><span class="p">(</span><span class="n">A_elems</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="go">array([[0., 0., 0., 0., 0.],</span>
<span class="go">       [0., 2., 4., 2., 0.],</span>
<span class="go">       [0., 4., 0., 4., 0.],</span>
<span class="go">       [0., 2., 4., 2., 0.],</span>
<span class="go">       [0., 0., 0., 0., 0.]])</span>
</pre></div>
</div>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="In this tutorial, we compute the structure tensor of a 3D image. For a general introduction to 3D image processing, please refer to sphx_glr_auto_examples_applications_plot_3d_image_processing.py. The data we use here are sampled from an image of kidney tissue obtained by confocal fluorescence microscopy (more details at [1]_ under kidney-tissue-fluorescence.tif)."><img alt="" src="../_images/sphx_glr_plot_3d_structure_tensor_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_3d_structure_tensor.html#sphx-glr-auto-examples-applications-plot-3d-structure-tensor-py"><span class="std std-ref">Estimate anisotropy in a 3D microscopy image</span></a></p>
  <div class="sphx-glr-thumbnail-title">Estimate anisotropy in a 3D microscopy image</div>
</div></div></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="skimage.feature.BRIEF">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">BRIEF</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">descriptor_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">49</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/brief.py#L15-L209"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.BRIEF" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">DescriptorExtractor</span></code></p>
<p>BRIEF binary descriptor extractor.</p>
<p>BRIEF (Binary Robust Independent Elementary Features) is an efficient
feature point descriptor. It is highly discriminative even when using
relatively few bits and is computed using simple intensity difference
tests.</p>
<p>For each keypoint, intensity comparisons are carried out for a specifically
distributed number N of pixel-pairs resulting in a binary descriptor of
length N. For binary descriptors the Hamming distance can be used for
feature matching, which leads to lower computational cost in comparison to
the L2 norm.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>descriptor_size</strong><span class="classifier">int, optional</span></dt><dd><p>Size of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512
recommended by the authors. Default is 256.</p>
</dd>
<dt><strong>patch_size</strong><span class="classifier">int, optional</span></dt><dd><p>Length of the two dimensional square patch sampling region around
the keypoints. Default is 49.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">{‘normal’, ‘uniform’}, optional</span></dt><dd><p>Probability distribution for sampling location of decision pixel-pairs
around keypoints.</p>
</dd>
<dt><strong>rng</strong><span class="classifier">{<a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator" title="(in NumPy v2.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.random.Generator</span></code></a>, int}, optional</span></dt><dd><p>Pseudo-random number generator (RNG).
By default, a PCG64 generator is used (see <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng" title="(in NumPy v2.1)"><code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.random.default_rng()</span></code></a>).
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">rng</span></code> is an int, it is used to seed the generator.</p>
<p>The PRNG is used for the random sampling of the decision
pixel-pairs. From a square window with length <code class="xref py py-obj docutils literal notranslate"><span class="pre">patch_size</span></code>,
pixel pairs are sampled using the <code class="xref py py-obj docutils literal notranslate"><span class="pre">mode</span></code> parameter to build
the descriptors using intensity comparison.</p>
<p>For matching across images, the same <code class="xref py py-obj docutils literal notranslate"><span class="pre">rng</span></code> should be used to construct
descriptors. To facilitate this:</p>
<ol class="loweralpha simple">
<li><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">rng</span></code> defaults to 1</p></li>
<li><p>Subsequent calls of the <code class="docutils literal notranslate"><span class="pre">extract</span></code> method will use the same rng/seed.</p></li>
</ol>
</dd>
<dt><strong>sigma</strong><span class="classifier">float, optional</span></dt><dd><p>Standard deviation of the Gaussian low-pass filter applied to the image
to alleviate noise sensitivity, which is strongly recommended to obtain
discriminative and good descriptors.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>descriptors</strong><span class="classifier">(Q, <code class="xref py py-obj docutils literal notranslate"><span class="pre">descriptor_size</span></code>) array of dtype bool</span></dt><dd><p>2D ndarray of binary descriptors of size <code class="xref py py-obj docutils literal notranslate"><span class="pre">descriptor_size</span></code> for Q
keypoints after filtering out border keypoints with value at an
index <code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> either being <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code> representing
the outcome of the intensity comparison for i-th keypoint on j-th
decision pixel-pair. It is <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">==</span> <span class="pre">np.sum(mask)</span></code>.</p>
</dd>
<dt><strong>mask</strong><span class="classifier">(N,) array of dtype bool</span></dt><dd><p>Mask indicating whether a keypoint has been filtered out
(<code class="docutils literal notranslate"><span class="pre">False</span></code>) or is described in the <code class="xref py py-obj docutils literal notranslate"><span class="pre">descriptors</span></code> array (<code class="docutils literal notranslate"><span class="pre">True</span></code>).</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="p">(</span><span class="n">corner_harris</span><span class="p">,</span> <span class="n">corner_peaks</span><span class="p">,</span> <span class="n">BRIEF</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">match_descriptors</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square1</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square1</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square2</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square2</span>
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 1, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keypoints1</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">square1</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keypoints2</span> <span class="o">=</span> <span class="n">corner_peaks</span><span class="p">(</span><span class="n">corner_harris</span><span class="p">(</span><span class="n">square2</span><span class="p">),</span> <span class="n">min_distance</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extractor</span> <span class="o">=</span> <span class="n">BRIEF</span><span class="p">(</span><span class="n">patch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extractor</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">square1</span><span class="p">,</span> <span class="n">keypoints1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">descriptors1</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">descriptors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extractor</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">square2</span><span class="p">,</span> <span class="n">keypoints2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">descriptors2</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">descriptors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span> <span class="o">=</span> <span class="n">match_descriptors</span><span class="p">(</span><span class="n">descriptors1</span><span class="p">,</span> <span class="n">descriptors2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span>
<span class="go">array([[0, 0],</span>
<span class="go">       [1, 1],</span>
<span class="go">       [2, 2],</span>
<span class="go">       [3, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keypoints1</span><span class="p">[</span><span class="n">matches</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 5],</span>
<span class="go">       [5, 2],</span>
<span class="go">       [5, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keypoints2</span><span class="p">[</span><span class="n">matches</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="go">array([[2, 2],</span>
<span class="go">       [2, 6],</span>
<span class="go">       [6, 2],</span>
<span class="go">       [6, 6]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.BRIEF.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">descriptor_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">49</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/brief.py#L126-L152"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.BRIEF.__init__" title="Link to this definition">#</a></dt>
<dd><div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the BRIEF binary description algorithm. The descriptor consists of relatively few bits and can be computed using a set of intensity difference tests. The short binary descriptor results in low memory footprint and very efficient matching based on the Hamming distance metric. BRIEF does not provide rotation-invariance. Scale-invariance can be achieved by detecting and extracting features at different scales."><img alt="" src="../_images/sphx_glr_plot_brief_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_brief.html#sphx-glr-auto-examples-features-detection-plot-brief-py"><span class="std std-ref">BRIEF binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">BRIEF binary descriptor</div>
</div></div></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.BRIEF.extract">
<span class="sig-name descname"><span class="pre">extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/brief.py#L154-L209"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.BRIEF.extract" title="Link to this definition">#</a></dt>
<dd><p>Extract BRIEF binary descriptors for given keypoints in image.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>keypoints</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="skimage.feature.CENSURE">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">CENSURE</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'DoB'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_max_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">line_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/censure.py#L146-L343"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.CENSURE" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureDetector</span></code></p>
<p>CENSURE keypoint detector.</p>
<dl class="simple">
<dt>min_scale<span class="classifier">int, optional</span></dt><dd><p>Minimum scale to extract keypoints from.</p>
</dd>
<dt>max_scale<span class="classifier">int, optional</span></dt><dd><p>Maximum scale to extract keypoints from. The keypoints will be
extracted from all the scales except the first and the last i.e.
from the scales in the range [min_scale + 1, max_scale - 1]. The filter
sizes for different scales is such that the two adjacent scales
comprise of an octave.</p>
</dd>
<dt>mode<span class="classifier">{‘DoB’, ‘Octagon’, ‘STAR’}, optional</span></dt><dd><p>Type of bi-level filter used to get the scales of the input image.
Possible values are ‘DoB’, ‘Octagon’ and ‘STAR’. The three modes
represent the shape of the bi-level filters i.e. box(square), octagon
and star respectively. For instance, a bi-level octagon filter consists
of a smaller inner octagon and a larger outer octagon with the filter
weights being uniformly negative in both the inner octagon while
uniformly positive in the difference region. Use STAR and Octagon for
better features and DoB for better performance.</p>
</dd>
<dt>non_max_threshold<span class="classifier">float, optional</span></dt><dd><p>Threshold value used to suppress maximas and minimas with a weak
magnitude response obtained after Non-Maximal Suppression.</p>
</dd>
<dt>line_threshold<span class="classifier">float, optional</span></dt><dd><p>Threshold for rejecting interest points which have ratio of principal
curvatures greater than this value.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>keypoints</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>scales</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding scales.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="recc8560c357f-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Motilal Agrawal, Kurt Konolige and Morten Rufus Blas
“CENSURE: Center Surround Extremas for Realtime Feature
Detection and Matching”,
<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8">https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8</a>
<a class="reference external" href="https://doi.org/10.1007/978-3-540-88693-8_8">DOI:10.1007/978-3-540-88693-8_8</a></p>
</div>
<div class="citation" id="recc8560c357f-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<p>Adam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala
“Comparative Assessment of Point Feature Detectors and
Descriptors in the Context of Robot Navigation”
<a class="reference external" href="http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf">http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf</a>
<a class="reference external" href="https://doi.org/10.1.1.465.1117">DOI:10.1.1.465.1117</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.data</span> <span class="kn">import</span> <span class="n">astronaut</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.color</span> <span class="kn">import</span> <span class="n">rgb2gray</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">CENSURE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">rgb2gray</span><span class="p">(</span><span class="n">astronaut</span><span class="p">()[</span><span class="mi">100</span><span class="p">:</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">:</span><span class="mi">300</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">censure</span> <span class="o">=</span> <span class="n">CENSURE</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">censure</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">censure</span><span class="o">.</span><span class="n">keypoints</span>
<span class="go">array([[  4, 148],</span>
<span class="go">       [ 12,  73],</span>
<span class="go">       [ 21, 176],</span>
<span class="go">       [ 91,  22],</span>
<span class="go">       [ 93,  56],</span>
<span class="go">       [ 94,  22],</span>
<span class="go">       [ 95,  54],</span>
<span class="go">       [100,  51],</span>
<span class="go">       [103,  51],</span>
<span class="go">       [106,  67],</span>
<span class="go">       [108,  15],</span>
<span class="go">       [117,  20],</span>
<span class="go">       [122,  60],</span>
<span class="go">       [125,  37],</span>
<span class="go">       [129,  37],</span>
<span class="go">       [133,  76],</span>
<span class="go">       [145,  44],</span>
<span class="go">       [146,  94],</span>
<span class="go">       [150, 114],</span>
<span class="go">       [153,  33],</span>
<span class="go">       [154, 156],</span>
<span class="go">       [155, 151],</span>
<span class="go">       [184,  63]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">censure</span><span class="o">.</span><span class="n">scales</span>
<span class="go">array([2, 6, 6, 2, 4, 3, 2, 3, 2, 6, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 4, 2,</span>
<span class="go">       2])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.CENSURE.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'DoB'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">non_max_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">line_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/censure.py#L232-L256"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.CENSURE.__init__" title="Link to this definition">#</a></dt>
<dd><div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="The CENSURE feature detector is a scale-invariant center-surround detector (CENSURE) that claims to outperform other detectors and is capable of real-time implementation."><img alt="" src="../_images/sphx_glr_plot_censure_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_censure.html#sphx-glr-auto-examples-features-detection-plot-censure-py"><span class="std std-ref">CENSURE feature detector</span></a></p>
  <div class="sphx-glr-thumbnail-title">CENSURE feature detector</div>
</div></div></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.CENSURE.detect">
<span class="sig-name descname"><span class="pre">detect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/censure.py#L258-L343"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.CENSURE.detect" title="Link to this definition">#</a></dt>
<dd><p>Detect CENSURE keypoints along with the corresponding scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D ndarray</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="skimage.feature.Cascade">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">Cascade</span></span><a class="headerlink" href="#skimage.feature.Cascade" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Class for cascade of classifiers that is used for object detection.</p>
<p>The main idea behind cascade of classifiers is to create classifiers
of medium accuracy and ensemble them into one strong classifier
instead of just creating a strong one. The second advantage of cascade
classifier is that easy examples can be classified only by evaluating
some of the classifiers in the cascade, making the process much faster
than the process of evaluating a one strong classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Attributes<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>eps</strong><span class="classifier">cnp.float32_t</span></dt><dd><p>Accuracy parameter. Increasing it, makes the classifier detect less
false positives but at the same time the false negative score increases.</p>
</dd>
<dt><strong>stages_number</strong><span class="classifier">Py_ssize_t</span></dt><dd><p>Amount of stages in a cascade. Each cascade consists of stumps i.e.
trained features.</p>
</dd>
<dt><strong>stumps_number</strong><span class="classifier">Py_ssize_t</span></dt><dd><p>The overall amount of stumps in all the stages of cascade.</p>
</dd>
<dt><strong>features_number</strong><span class="classifier">Py_ssize_t</span></dt><dd><p>The overall amount of different features used by cascade.
Two stumps can use the same features but has different trained
values.</p>
</dd>
<dt><strong>window_width</strong><span class="classifier">Py_ssize_t</span></dt><dd><p>The width of a detection window that is used. Objects smaller than
this window can’t be detected.</p>
</dd>
<dt><strong>window_height</strong><span class="classifier">Py_ssize_t</span></dt><dd><p>The height of a detection window.</p>
</dd>
<dt><strong>stages</strong><span class="classifier">Stage*</span></dt><dd><p>A pointer to the C array that stores stages information using a
Stage struct.</p>
</dd>
<dt><strong>features</strong><span class="classifier">MBLBP*</span></dt><dd><p>A pointer to the C array that stores MBLBP features using an MBLBP
struct.</p>
</dd>
<dt><strong>LUTs</strong><span class="classifier">cnp.uint32_t*</span></dt><dd><p>A pointer to the C array with look-up tables that are used by trained
MBLBP features (MBLBPStumps) to evaluate a particular region.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The cascade approach was first described by Viola and Jones <a class="reference internal" href="#rec331695a2d7-1" id="id69">[1]</a>, <a class="reference internal" href="#rec331695a2d7-2" id="id70">[2]</a>,
although these initial publications used a set of Haar-like features. This
implementation instead uses multi-scale block local binary pattern (MB-LBP)
features <a class="reference internal" href="#rec331695a2d7-3" id="id71">[3]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rec331695a2d7-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id69">1</a><span class="fn-bracket">]</span></span>
<p>Viola, P. and Jones, M. “Rapid object detection using a boosted
cascade of simple features,” In: Proceedings of the 2001 IEEE
Computer Society Conference on Computer Vision and Pattern
Recognition. CVPR 2001, pp. I-I.
<a class="reference external" href="https://doi.org/10.1109/CVPR.2001.990517">DOI:10.1109/CVPR.2001.990517</a></p>
</div>
<div class="citation" id="rec331695a2d7-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id70">2</a><span class="fn-bracket">]</span></span>
<p>Viola, P. and Jones, M.J, “Robust Real-Time Face Detection”,
International Journal of Computer Vision 57, 137–154 (2004).
<a class="reference external" href="https://doi.org/10.1023/B:VISI.0000013087.49260.fb">DOI:10.1023/B:VISI.0000013087.49260.fb</a></p>
</div>
<div class="citation" id="rec331695a2d7-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id71">3</a><span class="fn-bracket">]</span></span>
<p>Liao, S. et al. Learning Multi-scale Block Local Binary Patterns for
Face Recognition. International Conference on Biometrics (ICB),
2007, pp. 828-837. In: Lecture Notes in Computer Science, vol 4642.
Springer, Berlin, Heidelberg.
<a class="reference external" href="https://doi.org/10.1007/978-3-540-74549-5_87">DOI:10.1007/978-3-540-74549-5_87</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.Cascade.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#skimage.feature.Cascade.__init__" title="Link to this definition">#</a></dt>
<dd><p>Initialize cascade classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>xml_file</strong><span class="classifier">file’s path or file’s object</span></dt><dd><p>A file in a OpenCv format from which all the cascade classifier’s
parameters are loaded.</p>
</dd>
<dt><strong>eps</strong><span class="classifier">cnp.float32_t</span></dt><dd><p>Accuracy parameter. Increasing it, makes the classifier
detect less false positives but at the same time the false
negative score increases.</p>
</dd>
</dl>
</dd>
</dl>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This computer vision example shows how to detect faces on an image using object detection framework based on machine learning."><img alt="" src="../_images/sphx_glr_plot_face_detection_thumb.png" />
<p><a class="reference internal" href="../auto_examples/applications/plot_face_detection.html#sphx-glr-auto-examples-applications-plot-face-detection-py"><span class="std std-ref">Face detection using a cascade classifier</span></a></p>
  <div class="sphx-glr-thumbnail-title">Face detection using a cascade classifier</div>
</div></div></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.Cascade.detect_multi_scale">
<span class="sig-name descname"><span class="pre">detect_multi_scale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">img</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_ratio</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_neighbor_number</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intersection_score_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#skimage.feature.Cascade.detect_multi_scale" title="Link to this definition">#</a></dt>
<dd><p>Search for the object on multiple scales of input image.</p>
<p>The function takes the input image, the scale factor by which the
searching window is multiplied on each step, minimum window size
and maximum window size that specify the interval for the search
windows that are applied to the input image to detect objects.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>img</strong><span class="classifier">2-D or 3-D ndarray</span></dt><dd><p>Ndarray that represents the input image.</p>
</dd>
<dt><strong>scale_factor</strong><span class="classifier">cnp.float32_t</span></dt><dd><p>The scale by which searching window is multiplied on each step.</p>
</dd>
<dt><strong>step_ratio</strong><span class="classifier">cnp.float32_t</span></dt><dd><p>The ratio by which the search step in multiplied on each scale
of the image. 1 represents the exaustive search and usually is
slow. By setting this parameter to higher values the results will
be worse but the computation will be much faster. Usually, values
in the interval [1, 1.5] give good results.</p>
</dd>
<dt><strong>min_size</strong><span class="classifier">tuple (int, int)</span></dt><dd><p>Minimum size of the search window.</p>
</dd>
<dt><strong>max_size</strong><span class="classifier">tuple (int, int)</span></dt><dd><p>Maximum size of the search window.</p>
</dd>
<dt><strong>min_neighbor_number</strong><span class="classifier">int</span></dt><dd><p>Minimum amount of intersecting detections in order for detection
to be approved by the function.</p>
</dd>
<dt><strong>intersection_score_threshold</strong><span class="classifier">cnp.float32_t</span></dt><dd><p>The minimum value of value of ratio
(intersection area) / (small rectangle ratio) in order to merge
two detections into one.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>output</strong><span class="classifier">list of dicts</span></dt><dd><p>Dict have form {‘r’: int, ‘c’: int, ‘width’: int, ‘height’: int},
where ‘r’ represents row position of top left corner of detected
window, ‘c’ - col position, ‘width’ - width of detected window,
‘height’ - height of detected window.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><a class="headerlink" href="#skimage.feature.Cascade.eps" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.features_number">
<span class="sig-name descname"><span class="pre">features_number</span></span><a class="headerlink" href="#skimage.feature.Cascade.features_number" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.stages_number">
<span class="sig-name descname"><span class="pre">stages_number</span></span><a class="headerlink" href="#skimage.feature.Cascade.stages_number" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.stumps_number">
<span class="sig-name descname"><span class="pre">stumps_number</span></span><a class="headerlink" href="#skimage.feature.Cascade.stumps_number" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.window_height">
<span class="sig-name descname"><span class="pre">window_height</span></span><a class="headerlink" href="#skimage.feature.Cascade.window_height" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="skimage.feature.Cascade.window_width">
<span class="sig-name descname"><span class="pre">window_width</span></span><a class="headerlink" href="#skimage.feature.Cascade.window_width" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="skimage.feature.ORB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">ORB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">downscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_scales</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_keypoints</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">harris_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.04</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/orb.py#L25-L366"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.ORB" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureDetector</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DescriptorExtractor</span></code></p>
<p>Oriented FAST and rotated BRIEF feature detector and binary descriptor
extractor.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>n_keypoints</strong><span class="classifier">int, optional</span></dt><dd><p>Number of keypoints to be returned. The function will return the best
<code class="xref py py-obj docutils literal notranslate"><span class="pre">n_keypoints</span></code> according to the Harris corner response if more than
<code class="xref py py-obj docutils literal notranslate"><span class="pre">n_keypoints</span></code> are detected. If not, then all the detected keypoints
are returned.</p>
</dd>
<dt><strong>fast_n</strong><span class="classifier">int, optional</span></dt><dd><p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">n</span></code> parameter in <a class="reference internal" href="#skimage.feature.corner_fast" title="skimage.feature.corner_fast"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.corner_fast</span></code></a>. Minimum number of
consecutive pixels out of 16 pixels on the circle that should all be
either brighter or darker w.r.t test-pixel. A point c on the circle is
darker w.r.t test pixel p if <code class="docutils literal notranslate"><span class="pre">Ic</span> <span class="pre">&lt;</span> <span class="pre">Ip</span> <span class="pre">-</span> <span class="pre">threshold</span></code> and brighter if
<code class="docutils literal notranslate"><span class="pre">Ic</span> <span class="pre">&gt;</span> <span class="pre">Ip</span> <span class="pre">+</span> <span class="pre">threshold</span></code>. Also stands for the n in <code class="docutils literal notranslate"><span class="pre">FAST-n</span></code> corner
detector.</p>
</dd>
<dt><strong>fast_threshold</strong><span class="classifier">float, optional</span></dt><dd><p>The <code class="docutils literal notranslate"><span class="pre">threshold</span></code> parameter in <code class="docutils literal notranslate"><span class="pre">feature.corner_fast</span></code>. Threshold used
to decide whether the pixels on the circle are brighter, darker or
similar w.r.t. the test pixel. Decrease the threshold when more
corners are desired and vice-versa.</p>
</dd>
<dt><strong>harris_k</strong><span class="classifier">float, optional</span></dt><dd><p>The <code class="xref py py-obj docutils literal notranslate"><span class="pre">k</span></code> parameter in <a class="reference internal" href="#skimage.feature.corner_harris" title="skimage.feature.corner_harris"><code class="xref py py-obj docutils literal notranslate"><span class="pre">skimage.feature.corner_harris</span></code></a>. Sensitivity
factor to separate corners from edges, typically in range <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0.2]</span></code>.
Small values of <code class="xref py py-obj docutils literal notranslate"><span class="pre">k</span></code> result in detection of sharp corners.</p>
</dd>
<dt><strong>downscale</strong><span class="classifier">float, optional</span></dt><dd><p>Downscale factor for the image pyramid. Default value 1.2 is chosen so
that there are more dense scales which enable robust scale invariance
for a subsequent feature description.</p>
</dd>
<dt><strong>n_scales</strong><span class="classifier">int, optional</span></dt><dd><p>Maximum number of scales from the bottom of the image pyramid to
extract the features from.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>keypoints</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>scales</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding scales.</p>
</dd>
<dt><strong>orientations</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding orientations in radians.</p>
</dd>
<dt><strong>responses</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding Harris corner responses.</p>
</dd>
<dt><strong>descriptors</strong><span class="classifier">(Q, <code class="xref py py-obj docutils literal notranslate"><span class="pre">descriptor_size</span></code>) array of dtype bool</span></dt><dd><p>2D array of binary descriptors of size <code class="xref py py-obj docutils literal notranslate"><span class="pre">descriptor_size</span></code> for Q
keypoints after filtering out border keypoints with value at an
index <code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> either being <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code> representing
the outcome of the intensity comparison for i-th keypoint on j-th
decision pixel-pair. It is <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">==</span> <span class="pre">np.sum(mask)</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rb3ecaf5c48ec-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski
“ORB: An efficient alternative to SIFT and SURF”
<a class="reference external" href="http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf">http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">ORB</span><span class="p">,</span> <span class="n">match_descriptors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">img1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">19481137</span><span class="p">)</span>  <span class="c1"># do not copy this value</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span><span class="p">[</span><span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">,</span> <span class="mi">40</span><span class="p">:</span><span class="mi">60</span><span class="p">]</span> <span class="o">=</span> <span class="n">square</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span><span class="p">[</span><span class="mi">53</span><span class="p">:</span><span class="mi">73</span><span class="p">,</span> <span class="mi">53</span><span class="p">:</span><span class="mi">73</span><span class="p">]</span> <span class="o">=</span> <span class="n">square</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span> <span class="o">=</span> <span class="n">ORB</span><span class="p">(</span><span class="n">n_keypoints</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span> <span class="o">=</span> <span class="n">ORB</span><span class="p">(</span><span class="n">n_keypoints</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">detect_and_extract</span><span class="p">(</span><span class="n">img1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span><span class="o">.</span><span class="n">detect_and_extract</span><span class="p">(</span><span class="n">img2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span> <span class="o">=</span> <span class="n">match_descriptors</span><span class="p">(</span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">descriptors</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">detector_extractor2</span><span class="o">.</span><span class="n">descriptors</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span>
<span class="go">array([[0, 0],</span>
<span class="go">       [1, 1],</span>
<span class="go">       [2, 2],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">keypoints</span><span class="p">[</span><span class="n">matches</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="go">array([[59. , 59. ],</span>
<span class="go">       [40. , 40. ],</span>
<span class="go">       [57. , 40. ],</span>
<span class="go">       [46. , 58. ],</span>
<span class="go">       [58.8, 58.8]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span><span class="o">.</span><span class="n">keypoints</span><span class="p">[</span><span class="n">matches</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="go">array([[72., 72.],</span>
<span class="go">       [53., 53.],</span>
<span class="go">       [70., 53.],</span>
<span class="go">       [59., 71.],</span>
<span class="go">       [72., 72.]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.ORB.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">downscale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_scales</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_keypoints</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">harris_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.04</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/orb.py#L119-L139"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.ORB.__init__" title="Link to this definition">#</a></dt>
<dd><div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to robustly estimate epipolar geometry &lt;https://en.wikipedia.org/wiki/Epipolar_geometry&gt; (the geometry of stereo vision) between two views using sparse ORB feature correspondences."><img alt="" src="../_images/sphx_glr_plot_fundamental_matrix_thumb.png" />
<p><a class="reference internal" href="../auto_examples/transform/plot_fundamental_matrix.html#sphx-glr-auto-examples-transform-plot-fundamental-matrix-py"><span class="std std-ref">Fundamental matrix estimation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fundamental matrix estimation</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the ORB feature detection and binary description algorithm. It uses an oriented FAST detection method and the rotated BRIEF descriptors."><img alt="" src="../_images/sphx_glr_plot_orb_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_orb.html#sphx-glr-auto-examples-features-detection-plot-orb-py"><span class="std std-ref">ORB feature detector and binary descriptor</span></a></p>
  <div class="sphx-glr-thumbnail-title">ORB feature detector and binary descriptor</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A Fisher vector is an image feature encoding and quantization technique that can be seen as a soft or probabilistic version of the popular bag-of-visual-words or VLAD algorithms. Images are modelled using a visual vocabulary which is estimated using a K-mode Gaussian mixture model trained on low-level image features such as SIFT or ORB descriptors. The Fisher vector itself is a concatenation of the gradients of the Gaussian mixture model (GMM) with respect to its parameters - mixture weights, means, and covariance matrices."><img alt="" src="../_images/sphx_glr_plot_fisher_vector_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_fisher_vector.html#sphx-glr-auto-examples-features-detection-plot-fisher-vector-py"><span class="std std-ref">Fisher vector feature encoding</span></a></p>
  <div class="sphx-glr-thumbnail-title">Fisher vector feature encoding</div>
</div></div></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.ORB.detect">
<span class="sig-name descname"><span class="pre">detect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/orb.py#L172-L226"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.ORB.detect" title="Link to this definition">#</a></dt>
<dd><p>Detect oriented FAST keypoints along with the corresponding scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.ORB.detect_and_extract">
<span class="sig-name descname"><span class="pre">detect_and_extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/orb.py#L290-L366"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.ORB.detect_and_extract" title="Link to this definition">#</a></dt>
<dd><p>Detect oriented FAST keypoints and extract rBRIEF descriptors.</p>
<p>Note that this is faster than first calling <a class="reference internal" href="#skimage.feature.ORB.detect" title="skimage.feature.ORB.detect"><code class="xref py py-obj docutils literal notranslate"><span class="pre">detect</span></code></a> and then
<a class="reference internal" href="#skimage.feature.ORB.extract" title="skimage.feature.ORB.extract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extract</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.ORB.extract">
<span class="sig-name descname"><span class="pre">extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">orientations</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/orb.py#L239-L288"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.ORB.extract" title="Link to this definition">#</a></dt>
<dd><p>Extract rBRIEF binary descriptors for given keypoints in image.</p>
<p>Note that the keypoints must be extracted using the same <code class="xref py py-obj docutils literal notranslate"><span class="pre">downscale</span></code>
and <code class="xref py py-obj docutils literal notranslate"><span class="pre">n_scales</span></code> parameters. Additionally, if you want to extract both
keypoints and descriptors you should use the faster
<a class="reference internal" href="#skimage.feature.ORB.detect_and_extract" title="skimage.feature.ORB.detect_and_extract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">detect_and_extract</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
<dt><strong>keypoints</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>scales</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding scales.</p>
</dd>
<dt><strong>orientations</strong><span class="classifier">(N,) array</span></dt><dd><p>Corresponding orientations in radians.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="skimage.feature.SIFT">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">skimage.feature.</span></span><span class="sig-name descname"><span class="pre">SIFT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upsampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_octaves</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_scales</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_dog</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.013333333333333334</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_edge</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">36</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_ori</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_descr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ori</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/sift.py#L102-L771"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.SIFT" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureDetector</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DescriptorExtractor</span></code></p>
<p>SIFT feature detection and descriptor extraction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>upsampling</strong><span class="classifier">int, optional</span></dt><dd><p>Prior to the feature detection the image is upscaled by a factor
of 1 (no upscaling), 2 or 4. Method: Bi-cubic interpolation.</p>
</dd>
<dt><strong>n_octaves</strong><span class="classifier">int, optional</span></dt><dd><p>Maximum number of octaves. With every octave the image size is
halved and the sigma doubled. The number of octaves will be
reduced as needed to keep at least 12 pixels along each dimension
at the smallest scale.</p>
</dd>
<dt><strong>n_scales</strong><span class="classifier">int, optional</span></dt><dd><p>Maximum number of scales in every octave.</p>
</dd>
<dt><strong>sigma_min</strong><span class="classifier">float, optional</span></dt><dd><p>The blur level of the seed image. If upsampling is enabled
sigma_min is scaled by factor 1/upsampling</p>
</dd>
<dt><strong>sigma_in</strong><span class="classifier">float, optional</span></dt><dd><p>The assumed blur level of the input image.</p>
</dd>
<dt><strong>c_dog</strong><span class="classifier">float, optional</span></dt><dd><p>Threshold to discard low contrast extrema in the DoG. It’s final
value is dependent on n_scales by the relation:
final_c_dog = (2^(1/n_scales)-1) / (2^(1/3)-1) * c_dog</p>
</dd>
<dt><strong>c_edge</strong><span class="classifier">float, optional</span></dt><dd><p>Threshold to discard extrema that lie in edges. If H is the
Hessian of an extremum, its “edgeness” is described by
tr(H)²/det(H). If the edgeness is higher than
(c_edge + 1)²/c_edge, the extremum is discarded.</p>
</dd>
<dt><strong>n_bins</strong><span class="classifier">int, optional</span></dt><dd><p>Number of bins in the histogram that describes the gradient
orientations around keypoint.</p>
</dd>
<dt><strong>lambda_ori</strong><span class="classifier">float, optional</span></dt><dd><p>The window used to find the reference orientation of a keypoint
has a width of 6 * lambda_ori * sigma and is weighted by a
standard deviation of 2 * lambda_ori * sigma.</p>
</dd>
<dt><strong>c_max</strong><span class="classifier">float, optional</span></dt><dd><p>The threshold at which a secondary peak in the orientation
histogram is accepted as orientation</p>
</dd>
<dt><strong>lambda_descr</strong><span class="classifier">float, optional</span></dt><dd><p>The window used to define the descriptor of a keypoint has a width
of 2 * lambda_descr * sigma * (n_hist+1)/n_hist and is weighted by
a standard deviation of lambda_descr * sigma.</p>
</dd>
<dt><strong>n_hist</strong><span class="classifier">int, optional</span></dt><dd><p>The window used to define the descriptor of a keypoint consists of
n_hist * n_hist histograms.</p>
</dd>
<dt><strong>n_ori</strong><span class="classifier">int, optional</span></dt><dd><p>The number of bins in the histograms of the descriptor patch.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>delta_min</strong><span class="classifier">float</span></dt><dd><p>The sampling distance of the first octave. It’s final value is
1/upsampling.</p>
</dd>
<dt><strong>float_dtype</strong><span class="classifier">type</span></dt><dd><p>The datatype of the image.</p>
</dd>
<dt><strong>scalespace_sigmas</strong><span class="classifier">(n_octaves, n_scales + 3) array</span></dt><dd><p>The sigma value of all scales in all octaves.</p>
</dd>
<dt><strong>keypoints</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>positions</strong><span class="classifier">(N, 2) array</span></dt><dd><p>Subpixel-precision keypoint coordinates as <code class="docutils literal notranslate"><span class="pre">(row,</span> <span class="pre">col)</span></code>.</p>
</dd>
<dt><strong>sigmas</strong><span class="classifier">(N,) array</span></dt><dd><p>The corresponding sigma (blur) value of a keypoint.</p>
</dd>
<dt><strong>scales</strong><span class="classifier">(N,) array</span></dt><dd><p>The corresponding scale of a keypoint.</p>
</dd>
<dt><strong>orientations</strong><span class="classifier">(N,) array</span></dt><dd><p>The orientations of the gradient around every keypoint.</p>
</dd>
<dt><strong>octaves</strong><span class="classifier">(N,) array</span></dt><dd><p>The corresponding octave of a keypoint.</p>
</dd>
<dt><strong>descriptors</strong><span class="classifier">(N, n_hist*n_hist*n_ori) array</span></dt><dd><p>The descriptors of a keypoint.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The SIFT algorithm was developed by David Lowe <a class="reference internal" href="#rdc36d83fc179-1" id="id76">[1]</a>, <a class="reference internal" href="#rdc36d83fc179-2" id="id77">[2]</a> and later
patented by the University of British Columbia. Since the patent expired in
2020 it’s free to use. The implementation here closely follows the
detailed description in <a class="reference internal" href="#rdc36d83fc179-3" id="id78">[3]</a>, including use of the same default parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rdc36d83fc179-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id76">1</a><span class="fn-bracket">]</span></span>
<p>D.G. Lowe. “Object recognition from local scale-invariant
features”, Proceedings of the Seventh IEEE International
Conference on Computer Vision, 1999, vol.2, pp. 1150-1157.
<a class="reference external" href="https://doi.org/10.1109/ICCV.1999.790410">DOI:10.1109/ICCV.1999.790410</a></p>
</div>
<div class="citation" id="rdc36d83fc179-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id77">2</a><span class="fn-bracket">]</span></span>
<p>D.G. Lowe. “Distinctive Image Features from Scale-Invariant
Keypoints”, International Journal of Computer Vision, 2004,
vol. 60, pp. 91–110.
<a class="reference external" href="https://doi.org/10.1023/B:VISI.0000029664.99615.94">DOI:10.1023/B:VISI.0000029664.99615.94</a></p>
</div>
<div class="citation" id="rdc36d83fc179-3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id78">3</a><span class="fn-bracket">]</span></span>
<p>I. R. Otero and M. Delbracio. “Anatomy of the SIFT Method”,
Image Processing On Line, 4 (2014), pp. 370–396.
<a class="reference external" href="https://doi.org/10.5201/ipol.2014.82">DOI:10.5201/ipol.2014.82</a></p>
</div>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.feature</span> <span class="kn">import</span> <span class="n">SIFT</span><span class="p">,</span> <span class="n">match_descriptors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.data</span> <span class="kn">import</span> <span class="n">camera</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skimage.transform</span> <span class="kn">import</span> <span class="n">rotate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">camera</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">rotate</span><span class="p">(</span><span class="n">camera</span><span class="p">(),</span> <span class="mi">90</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span> <span class="o">=</span> <span class="n">SIFT</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span> <span class="o">=</span> <span class="n">SIFT</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">detect_and_extract</span><span class="p">(</span><span class="n">img1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span><span class="o">.</span><span class="n">detect_and_extract</span><span class="p">(</span><span class="n">img2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span> <span class="o">=</span> <span class="n">match_descriptors</span><span class="p">(</span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">descriptors</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">detector_extractor2</span><span class="o">.</span><span class="n">descriptors</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">max_ratio</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matches</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="p">]</span>
<span class="go">array([[ 10, 412],</span>
<span class="go">       [ 11, 417],</span>
<span class="go">       [ 12, 407],</span>
<span class="go">       [ 13, 411],</span>
<span class="go">       [ 14, 406]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor1</span><span class="o">.</span><span class="n">keypoints</span><span class="p">[</span><span class="n">matches</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="go">array([[ 95, 214],</span>
<span class="go">       [ 97, 211],</span>
<span class="go">       [ 97, 218],</span>
<span class="go">       [102, 215],</span>
<span class="go">       [104, 218]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detector_extractor2</span><span class="o">.</span><span class="n">keypoints</span><span class="p">[</span><span class="n">matches</span><span class="p">[</span><span class="mi">10</span><span class="p">:</span><span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="go">array([[297,  95],</span>
<span class="go">       [301,  97],</span>
<span class="go">       [294,  97],</span>
<span class="go">       [297, 102],</span>
<span class="go">       [293, 104]])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.SIFT.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">upsampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_octaves</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_scales</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma_in</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_dog</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.013333333333333334</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_edge</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">36</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_ori</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_descr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_hist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_ori</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/sift.py#L233-L274"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.SIFT.__init__" title="Link to this definition">#</a></dt>
<dd><div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates the SIFT feature detection and its description algorithm."><img alt="" src="../_images/sphx_glr_plot_sift_thumb.png" />
<p><a class="reference internal" href="../auto_examples/features_detection/plot_sift.html#sphx-glr-auto-examples-features-detection-plot-sift-py"><span class="std std-ref">SIFT feature detector and descriptor extractor</span></a></p>
  <div class="sphx-glr-thumbnail-title">SIFT feature detector and descriptor extractor</div>
</div></div></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="skimage.feature.SIFT.deltas">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">deltas</span></span><a class="headerlink" href="#skimage.feature.SIFT.deltas" title="Link to this definition">#</a></dt>
<dd><p>The sampling distances of all octaves</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.SIFT.detect">
<span class="sig-name descname"><span class="pre">detect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/sift.py#L704-L727"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.SIFT.detect" title="Link to this definition">#</a></dt>
<dd><p>Detect the keypoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.SIFT.detect_and_extract">
<span class="sig-name descname"><span class="pre">detect_and_extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/sift.py#L746-L771"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.SIFT.detect_and_extract" title="Link to this definition">#</a></dt>
<dd><p>Detect the keypoints and extract their descriptors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="skimage.feature.SIFT.extract">
<span class="sig-name descname"><span class="pre">extract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/scikit-image/scikit-image/blob/v0.25.0rc1/skimage/feature/sift.py#L729-L744"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#skimage.feature.SIFT.extract" title="Link to this definition">#</a></dt>
<dd><p>Extract the descriptors for all keypoints in the image.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image</strong><span class="classifier">2D array</span></dt><dd><p>Input image.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


                </article>
              
              
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.blob_dog"><code class="docutils literal notranslate"><span class="pre">blob_dog()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.blob_doh"><code class="docutils literal notranslate"><span class="pre">blob_doh()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.blob_log"><code class="docutils literal notranslate"><span class="pre">blob_log()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.canny"><code class="docutils literal notranslate"><span class="pre">canny()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_fast"><code class="docutils literal notranslate"><span class="pre">corner_fast()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_foerstner"><code class="docutils literal notranslate"><span class="pre">corner_foerstner()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_harris"><code class="docutils literal notranslate"><span class="pre">corner_harris()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_kitchen_rosenfeld"><code class="docutils literal notranslate"><span class="pre">corner_kitchen_rosenfeld()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_moravec"><code class="docutils literal notranslate"><span class="pre">corner_moravec()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_orientations"><code class="docutils literal notranslate"><span class="pre">corner_orientations()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_peaks"><code class="docutils literal notranslate"><span class="pre">corner_peaks()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_shi_tomasi"><code class="docutils literal notranslate"><span class="pre">corner_shi_tomasi()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.corner_subpix"><code class="docutils literal notranslate"><span class="pre">corner_subpix()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.daisy"><code class="docutils literal notranslate"><span class="pre">daisy()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.draw_haar_like_feature"><code class="docutils literal notranslate"><span class="pre">draw_haar_like_feature()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.draw_multiblock_lbp"><code class="docutils literal notranslate"><span class="pre">draw_multiblock_lbp()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.fisher_vector"><code class="docutils literal notranslate"><span class="pre">fisher_vector()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.graycomatrix"><code class="docutils literal notranslate"><span class="pre">graycomatrix()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.graycoprops"><code class="docutils literal notranslate"><span class="pre">graycoprops()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.haar_like_feature"><code class="docutils literal notranslate"><span class="pre">haar_like_feature()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.haar_like_feature_coord"><code class="docutils literal notranslate"><span class="pre">haar_like_feature_coord()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.hessian_matrix"><code class="docutils literal notranslate"><span class="pre">hessian_matrix()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.hessian_matrix_det"><code class="docutils literal notranslate"><span class="pre">hessian_matrix_det()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.hessian_matrix_eigvals"><code class="docutils literal notranslate"><span class="pre">hessian_matrix_eigvals()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.hog"><code class="docutils literal notranslate"><span class="pre">hog()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.learn_gmm"><code class="docutils literal notranslate"><span class="pre">learn_gmm()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.local_binary_pattern"><code class="docutils literal notranslate"><span class="pre">local_binary_pattern()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.match_descriptors"><code class="docutils literal notranslate"><span class="pre">match_descriptors()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.match_template"><code class="docutils literal notranslate"><span class="pre">match_template()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.multiblock_lbp"><code class="docutils literal notranslate"><span class="pre">multiblock_lbp()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.multiscale_basic_features"><code class="docutils literal notranslate"><span class="pre">multiscale_basic_features()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.peak_local_max"><code class="docutils literal notranslate"><span class="pre">peak_local_max()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.plot_matched_features"><code class="docutils literal notranslate"><span class="pre">plot_matched_features()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.shape_index"><code class="docutils literal notranslate"><span class="pre">shape_index()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.structure_tensor"><code class="docutils literal notranslate"><span class="pre">structure_tensor()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.structure_tensor_eigenvalues"><code class="docutils literal notranslate"><span class="pre">structure_tensor_eigenvalues()</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.BRIEF"><code class="docutils literal notranslate"><span class="pre">BRIEF</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.BRIEF.__init__"><code class="docutils literal notranslate"><span class="pre">BRIEF.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.BRIEF.extract"><code class="docutils literal notranslate"><span class="pre">BRIEF.extract()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.CENSURE"><code class="docutils literal notranslate"><span class="pre">CENSURE</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.CENSURE.__init__"><code class="docutils literal notranslate"><span class="pre">CENSURE.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.CENSURE.detect"><code class="docutils literal notranslate"><span class="pre">CENSURE.detect()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade"><code class="docutils literal notranslate"><span class="pre">Cascade</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.__init__"><code class="docutils literal notranslate"><span class="pre">Cascade.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.detect_multi_scale"><code class="docutils literal notranslate"><span class="pre">Cascade.detect_multi_scale()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.eps"><code class="docutils literal notranslate"><span class="pre">Cascade.eps</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.features_number"><code class="docutils literal notranslate"><span class="pre">Cascade.features_number</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.stages_number"><code class="docutils literal notranslate"><span class="pre">Cascade.stages_number</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.stumps_number"><code class="docutils literal notranslate"><span class="pre">Cascade.stumps_number</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.window_height"><code class="docutils literal notranslate"><span class="pre">Cascade.window_height</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.Cascade.window_width"><code class="docutils literal notranslate"><span class="pre">Cascade.window_width</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.ORB"><code class="docutils literal notranslate"><span class="pre">ORB</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.ORB.__init__"><code class="docutils literal notranslate"><span class="pre">ORB.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.ORB.detect"><code class="docutils literal notranslate"><span class="pre">ORB.detect()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.ORB.detect_and_extract"><code class="docutils literal notranslate"><span class="pre">ORB.detect_and_extract()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.ORB.extract"><code class="docutils literal notranslate"><span class="pre">ORB.extract()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT"><code class="docutils literal notranslate"><span class="pre">SIFT</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT.__init__"><code class="docutils literal notranslate"><span class="pre">SIFT.__init__()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT.deltas"><code class="docutils literal notranslate"><span class="pre">SIFT.deltas</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT.detect"><code class="docutils literal notranslate"><span class="pre">SIFT.detect()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT.detect_and_extract"><code class="docutils literal notranslate"><span class="pre">SIFT.detect_and_extract()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#skimage.feature.SIFT.extract"><code class="docutils literal notranslate"><span class="pre">SIFT.extract()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/api/skimage.feature.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2013-2024, the scikit-image team.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.0.2.
    <br/>
  </p>
</div>
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>